## 🧠 **2-2. 텍스트 파운데이션 모델**

### 1️⃣ 파운데이션 모델이란?

**Foundation Model = AI의 기반이 되는 초거대 범용 모델**

* 새로운 태스크마다 별도 학습 필요 X
* 프롬프트(자연어 지시)만으로 다양한 작업 수행 가능

**예시**

* ChatGPT → 텍스트 파운데이션 모델
* SORA → 비디오 파운데이션 모델
* GPT-4o → 멀티모달 입력(텍스트·이미지·오디오), 멀티모달 출력

**3대 구성요소**

| 구성요소                            | 설명                  |
| ------------------------------- | ------------------- |
| **빅데이터**                        | 인터넷 전역의 대규모 텍스트로 학습 |
| **자가학습(Self-supervised)**       | 정답 라벨 없이 다음 토큰 예측   |
| **Transformer 기반 Attention 구조** | 병렬 학습·장기 문맥 처리 가능   |

---

### 2️⃣ 거대 언어모델(LLM, Large Language Model)

* **기존 언어모델 + 규모 확장**

  * 7B~175B 이상의 파라미터
  * 1조(T) 단위 토큰 데이터로 학습
* **Scaling Law (규모의 법칙):**

  * 모델 크기↑, 데이터↑, 학습 시간↑ → 성능↑
* **Emergent Property (창발성):**

  * 일정 규모를 넘으면 갑자기 새로운 능력 출현
  * 예: In-context learning, 추론(reasoning)

**주요 모델**

* ChatGPT(OpenAI)
* Claude(Anthropic)
* Gemini(Google)
* LLaMA2(Meta) / Gemma(Google) / Qwen(Alibaba)

---

### 3️⃣ LLM의 학습 방식

#### 🔹 기본 학습: **다음 토큰 예측 (Next Token Prediction)**

* 입력 시퀀스 `[x1, x2, … xl]` → 다음 토큰 `xl+1` 예측
* 대규모 코퍼스(수백~수천억 토큰)로 학습

#### 🔹 정렬 학습(Alignment)

> 모델의 출력이 **사람의 의도와 가치에 맞게** 조정되도록 추가 학습

1️⃣ **지시학습 (Instruction Tuning)**

* “명령형 데이터셋”을 통한 지도 미세조정(SFT)
* 모든 NLP 태스크를 “지시 + 응답” 포맷으로 변환
* 예시: **FLAN (Fine-tuned Language Net)**
* 효과: Zero-shot 성능 향상
* 한계: 정답이 명확하지 않은 개방형 문제에 약함

2️⃣ **선호학습 (Preference Learning)**

* 여러 응답 중 **사람이 더 선호하는 답**을 따르도록 학습
* 핵심 알고리즘: **RLHF (Reinforcement Learning from Human Feedback)**

🧩 **RLHF 3단계**

| 단계          | 내용                              |
| ----------- | ------------------------------- |
| (1) 지시학습    | SFT로 기본 응답 패턴 학습                |
| (2) 보상모델 학습 | 사람 선호 데이터를 기반으로 Reward Model 훈련 |
| (3) 강화학습    | 보상 높은 응답을 생성하도록 모델 조정           |

* 결과: 더 안전하고 덜 유해한 응답 생성 (Hallucination 감소)

---

### 4️⃣ LLM의 추론 (Inference)

#### 🔹 **Auto-regressive Generation**

* 한 단어씩 순차적으로 생성 (EOS 토큰까지 반복)

#### 🔹 **디코딩(Decoding) 알고리즘**

| 알고리즘                         | 특징                            |
| ---------------------------- | ----------------------------- |
| **Greedy**                   | 확률 가장 높은 토큰 선택 (단조로움)         |
| **Beam Search**              | k개 후보 동시 탐색 (품질↑, 속도↓)        |
| **Sampling**                 | 확률에 비례해 랜덤 선택 (다양성↑)          |
| **Temperature**              | T↑ 다양성↑ / T↓ 보수적              |
| **Top-K Sampling**           | 상위 K개 후보 중 확률 기반 샘플링          |
| **Top-P (Nucleus) Sampling** | 누적 확률 P 이내 후보만 고려 (품질·다양성 균형) |

---

### 5️⃣ 프롬프트 엔지니어링 (Prompt Engineering)

> “모델에게 올바른 질문을 던지는 기술”

* 입력 구조 = **지시(instruction) + 예시(few-shot examples)**
* **Chain-of-Thought(CoT)**:

  * “추론 과정”을 포함한 예시로 모델의 reasoning 유도
  * Google PaLM에서 추론 성능 크게 향상
* **Zero-shot CoT (“Step by step”)**:

  * 예시 없이 스스로 사고 단계를 생성하도록 유도

---

### 6️⃣ 거대 언어모델의 평가 (Evaluation)

| 평가 유형       | 방법                     | 예시               |
| ----------- | ---------------------- | ---------------- |
| **정답형 태스크** | 예측값과 정답 비교 → **정확도**   | MMLU(57개 분야 객관식) |
| **생성형 태스크** | 유사도, Perplexity(PPL) 등 | 요약, 번역, 스토리 생성   |
| **선호형 평가**  | 사람이 여러 응답을 비교          | LMArena          |

#### 🔹 LLM-as-Judge (G-Eval)

* LLM이 직접 생성문을 평가 (자기평가자)
* 장점: 인간과 유사한 평가 결과
* 한계:

  * 위치 편향(Position Bias)
  * 길이 편향(긴 응답 선호)
  * 자기 선호(Self-Preference)

---

### 7️⃣ 응용과 한계

#### 💡 응용

* **멀티모달 모델** (GPT-4o 등): 텍스트·이미지·음성 통합
* **합성 데이터 생성(Self-Instruct, Alpaca)**

  * GPT-3으로 5만+ 인공 데이터 생성 → LLaMA 기반 모델 학습
  * 필터링을 통해 더 높은 품질 달성

#### ⚠️ 한계

| 한계                     | 원인                     | 해결 방안            |
| ---------------------- | ---------------------- | ---------------- |
| **Hallucination (환각)** | 확률적 예측 구조, 불완전한 학습 데이터 | 검색 증강(RAG)       |
| **Jailbreaking (탈옥)**  | 프롬프트 조작으로 안전 장치 우회     | 방어 프롬프트·안전 필터 연구 |
| **AI-Text Detection**  | AI 생성 텍스트 탐지 필요        | 통계·LLM 기반 검출기    |

---

### 🧩 **핵심 개념 맵**

```
Foundation Model
 ├─ 데이터 확장 (Big Data)
 ├─ 학습 확장 (Self-supervised)
 ├─ 모델 확장 (Transformer 기반)
 ├─ Alignment (지시학습 + 선호학습)
 ├─ Decoding (Greedy, Beam, Sampling, Top-P)
 ├─ Prompting (CoT, Zero-shot CoT)
 ├─ Evaluation (MMLU, LLM-as-Judge)
 └─ 한계 (Hallucination, Jailbreak, Detection)
```
