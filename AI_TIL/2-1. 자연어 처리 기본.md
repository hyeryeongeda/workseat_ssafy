## 🧠 **2-1. 자연어 처리 기본 요약**

### 1️⃣ 단어를 숫자로 표현하기: **워드 임베딩 (Word Embedding)**

* **원-핫 인코딩 (One-hot encoding)**

  * 단어 하나를 1, 나머지는 0으로 표현
  * ✅ 단점: 유사도 정보 없음, 차원의 저주(고차원 희소 벡터)
* **워드 임베딩 (Word2Vec, 2013)**

  * 단어의 의미를 **밀집(dense)** 벡터로 표현
  * 비슷한 의미의 단어는 공간상 가깝게 위치
  * 학습 방법:

    * **Skip-Gram (SG):** 중심 단어 → 주변 단어 예측
    * **CBOW:** 주변 단어 → 중심 단어 예측

---

### 2️⃣ 순차적 데이터와 RNN

* **순차적 데이터(Sequential data)**

  * 순서가 중요하며, 길이가 가변적 (ex: 텍스트, 오디오)
* **RNN (Recurrent Neural Network)**

  * 이전 입력 정보를 기억(hidden state 유지)하며 순차 데이터 처리
  * 동일한 가중치 공유 → 파라미터 효율적
  * **한계:**

    * 기울기 소실(Gradient Vanishing)
    * 기울기 폭발(Gradient Explosion)

---

### 3️⃣ **LSTM (Long Short-Term Memory)**

* RNN의 한계를 해결한 모델
* **Cell State**: 장기 기억을 저장
* **3가지 게이트**

  * Forget Gate: 이전 정보 중 버릴 것 결정
  * Input Gate: 새 정보 중 저장할 것 결정
  * Output Gate: 다음 단계로 전달할 정보 결정

---

### 4️⃣ **자연어 생성 모델 (Seq2Seq, Attention)**

* **Seq2Seq (2014, Google)**

  * **Encoder–Decoder 구조**

    * Encoder: 입력 문장을 벡터로 인코딩
    * Decoder: 해당 벡터를 바탕으로 출력 문장 생성
  * 학습 기법:

    * Teacher Forcing (정답 단어를 강제로 넣어 학습 안정화)
  * **한계:**

    * 인코더의 마지막 hidden state에 모든 정보 압축 → **Bottleneck 문제**

* **Attention**

  * Decoder가 인코더의 전체 hidden state를 **가중합으로 직접 참조**
  * Bottleneck 문제 해결
  * **효과:**

    * 장기 의존성 개선
    * 정보 손실 감소
    * 해석 가능성(어디에 집중했는지 시각화 가능)

---

### 5️⃣ **Transformer (2017, Google — Attention is All You Need)**

* **핵심 구조:** Self-Attention 기반 Encoder–Decoder
* **Self-Attention**

  * Query, Key, Value 벡터를 사용해 문장 내 단어 간 상호작용 학습
  * 병렬처리 가능, 장기 의존성 학습 효율적
* **구성요소**

  * **Multi-Head Attention:** 여러 관점에서 Attention 수행
  * **Scaled Dot Product:** 내적값 안정화
  * **Residual Connection:** 기울기 소실 방지
  * **Layer Normalization:** 학습 안정화
  * **Positional Encoding:** 순서 정보 보완
  * **Masked Self-Attention:** 미래 단어 참조 방지
* **결과:**

  * 병렬 학습으로 효율적
  * 번역, 요약, 대화 등 다양한 NLP에 표준 구조로 자리잡음

---

### 6️⃣ **사전학습 기반 언어모델 (Pre-trained Language Models)**

#### 💡 사전학습(Pre-training)

* 대규모 데이터로 일반적인 언어 패턴을 학습한 뒤
  다양한 태스크에 **파인튜닝(Fine-tuning)**
* 워드임베딩보다 강력한 문맥 이해력

#### (1) **Encoder 기반 – BERT (2018, Google)**

* 양방향 문맥을 동시에 학습
* **Masked Language Modeling (MLM)**

  * 입력 단어 15%를 [MASK]로 치환 → 복원 예측
* **Next Sentence Prediction (NSP)**

  * 문장 간 관계 예측
* ✅ 장점: 문맥 이해, 문장/토큰 수준 태스크에 강함
* ❌ 한계: 생성 태스크에는 부적합

#### (2) **Encoder–Decoder 기반 – T5 (2019, Google)**

* 모든 태스크를 **Text-to-Text** 형태로 통합
* **Span Corruption**

  * 일부 연속된 토큰을 제거하고 복원하도록 학습
* ✅ 번역, 요약, QA 등 범용성 우수

#### (3) **Decoder 기반 – GPT 시리즈**

* **GPT-1 (2018)**

  * Transformer Decoder 구조, Autoregressive LM
* **GPT-2 (2019)**

  * 대규모 데이터로 자연스러운 생성 가능
* **GPT-3 (2020)**

  * 파인튜닝 없이 예시만 보고 태스크 수행
  * 👉 **In-Context Learning (ICL)**

    * 예시 0개: Zero-shot
    * 예시 1개: One-shot
    * 예시 몇 개: Few-shot

---

### 7️⃣ **In-Context Learning과 Prompting**

* **Chain-of-Thought (CoT) Prompting**

  * 모델이 문제 해결 과정을 **논리적 단계(step-by-step)** 로 서술
  * Few-shot보다 높은 성능 달성
* **Zero-shot CoT**

  * 예시 없이 “Step by Step” 문장만으로 추론 유도

---

## 🧩 **핵심 비교 정리**

| 구분              | 모델 구조             | 특징                      | 대표 모델           |
| --------------- | ----------------- | ----------------------- | --------------- |
| **RNN**         | 순환 구조             | 순서 학습 가능, 기울기 소실        | 기본 RNN          |
| **LSTM**        | 셀+게이트 구조          | 장기 의존성 해결               | LSTM            |
| **Seq2Seq**     | Encoder–Decoder   | 문장 변환 가능, Bottleneck 존재 | NMT             |
| **Attention**   | 가중합 메커니즘          | 정보 선택 집중                | Bahdanau, Luong |
| **Transformer** | 병렬 Self-Attention | 빠르고 효율적                 | BERT, GPT, T5   |
| **BERT**        | Encoder           | 이해 중심                   | 분류, QA          |
| **T5**          | Encoder–Decoder   | Text-to-Text            | 요약, 번역          |
| **GPT**         | Decoder           | 생성 중심                   | 대화, 작문          |

---
