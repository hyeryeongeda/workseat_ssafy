## 🧠 **3-2. 이미지 파운데이션 모델**

---

### 1️⃣ **파운데이션 모델(Foundation Model) 개념**

* **정의:**
  대규모 데이터를 광범위하게 학습해, 다양한 문제에 빠르게 적응할 수 있는 범용 AI 모델
  (예: GPT, CLIP, LLaVA, Gemini 등)

* **특징**

  * 트랜스포머 기반 + 자가지도(Self-supervised) 학습
  * 비지도 대규모 학습을 통한 **범용성**
  * 적은 데이터로도 높은 **전이학습 성능(Fine-tuning 성능)**
  * Zero-shot / Few-shot 대응 가능
  * 출력의 범위가 넓고 유연 (수천~만 개 클래스 분류 가능)

* **활용 방식**

  | 방식              | 특징                  |
  | --------------- | ------------------- |
  | **Zero-shot**   | 예시 없이도 문제 해결        |
  | **Few-shot**    | 예시 몇 개로 패턴 학습       |
  | **Fine-tuning** | 특정 작업에 맞게 가중치 일부 갱신 |

---

### 2️⃣ **컴퓨터 비전 파운데이션 모델들**

#### 📸 (1) 영상 파운데이션 모델

* 방대한 이미지 데이터를 학습한 범용 시각 모델
* 예: **분할(Segmentation)**, **탐지(Detection)**, **3D/Depth 추정** 등 다중 과업 수행 가능

---

#### 🧩 (2) 이미지–텍스트 관계 모델: **CLIP (2021, OpenAI)**

* **개념:**
  이미지와 텍스트의 의미적 유사성을 학습하는 모델
  (“이미지와 문장을 같은 임베딩 공간에 매핑”)

* **학습 데이터:**
  4억 개의 이미지–텍스트 쌍

* **구조:**

  * **이미지 인코더:** ViT or ResNet
  * **텍스트 인코더:** Transformer

* **학습 방식:**

  * **대조학습(Contrastive Learning)**
    → 올바른 짝(Positive)은 가깝게, 잘못된 짝(Negative)은 멀게

* **성과:**

  * Zero-shot 분류 가능
  * ViT 기반으로 기존 CNN보다 높은 성능
  * 이후 멀티모달 모델(VLM)들의 “눈 역할”이 됨

---

#### ⚙️ (3) CLIP 후속 – **SigLIP (2023, Google)**

* CLIP의 Softmax 대신 **Sigmoid Loss** 사용
* 음성 데이터에 의한 학습 불안정성 개선
* 메모리 효율 및 정확도 향상
* 최신 VLM(Vision-Language Model)의 표준 베이스

---

#### 🧠 (4) 멀티모달 언어모델: **LLaVA (2023)**

* **구조:** Vision Encoder + Language Model
* **학습 방식:**

  * 1단계: 이미지→텍스트 표현 변환 학습
  * 2단계: Instruction tuning으로 세부 미세조정
* **특징:**

  * 이미지 설명 생성(Image Captioning)
  * 시각적 질의응답(VQA)
  * GPT 기반 합성데이터 활용
  * 효율적 학습 (FP16, 저비용 Fine-tuning)

---

#### 🏥 (5) 도메인 특화 모델

| 분야        | 모델                     | 특징                        |
| --------- | ---------------------- | ------------------------- |
| **의료**    | **MedCLIP, LLaVA-Med** | 의료 영상(X-ray, MRI)과 텍스트 정합 |
| **제조업**   | **AnomalyGPT (2023)**  | 이미지 내 결함 탐지 + 질의응답 기능     |
| **3D 인식** | **3D-LLM**             | 점군(Point Cloud)–언어 매핑 학습  |

---

#### 🧩 (6) 주요 비전 모델

| 과업             | 대표 모델                     | 특징                 |
| -------------- | ------------------------- | ------------------ |
| **세그멘테이션**     | **SAM / SAM2 (Meta)**     | 클릭 기반 고정확도 분할      |
| **탐지**         | **Grounding DINO (IDEA)** | 텍스트 조건 기반 탐지       |
| **인스턴스 탐지+분할** | **Grounded SAM (2024)**   | SAM + Grounding 결합 |
| **비디오 탐지**     | **SAMURAI (2024)**        | 시간 정보 포함 탐지·분할     |

---

#### 🤖 (7) 로봇용 텍스트–행동 변환 모델

* 입력: 텍스트 명령 + 로봇 카메라 시점 영상
* 출력: 로봇의 실제 행동(움직임, 관절값 등)
* 비전–언어 모델을 통해 **로봇 제어 자동화**

---

### 3️⃣ **영상·3D 파운데이션 모델**

#### 🎨 (1) 영상 생성 모델

| 구분              | 대표 모델                        | 특징           |
| --------------- | ---------------------------- | ------------ |
| **이미지 생성**      | DALL·E3, Midjourney v7, FLUX | 텍스트-투-이미지    |
| **비디오 생성**      | SORA(OpenAI), Veo2(Google)   | 동영상 생성·편집    |
| **오픈소스 비디오 생성** | Hunyuan(Tencent)             | 텍스트 기반 영상 생성 |

---

#### 🧱 (2) 3D/비전 확장형 모델

| 모델                       | 설명                |
| ------------------------ | ----------------- |
| **Depth Anything v2**    | 깊이(Depth) 추정      |
| **Zero123XL**            | 단일 이미지로 새로운 시점 생성 |
| **JointDiT**             | 이미지 & 3D 동시 생성    |
| **MegaSaM / CUT3R**      | 비디오 기반 3D 복원      |
| **Sapiens / CLIP-Actor** | 사람 중심 3D 모델       |

---

### 4️⃣ **오디오–비전 파운데이션 모델**

* 멀티모달 확장형 언어모델
* **대표 예시**

  * **OneLLM (2024)**: 비디오+오디오 통합
  * **Video-LLaMA2 (2024)**: 프레임 단위 분석
  * **NExT-GPT (2023)**: Any-to-Any 멀티모달
  * **HeyGen Avatar IV**: 비디오 생성형 아바타 모델

---

### 5️⃣ **경량·온디바이스 이미지 모델 (sVLM 계열)**

| 모델                  | 개발 주체        | 특징                   |
| ------------------- | ------------ | -------------------- |
| **SmolVLM**         | HuggingFace  | 초소형 VLM, 실시간 추론      |
| **Moondream 0.5B**  | —            | 모바일 실시간 이미지 이해       |
| **Gemini Nano**     | Google       | 경량 Gemini, Edge AI용  |
| **갤럭시 온디바이스 AI**    | Samsung      | 기기 내 NPU로 AI 실행      |
| **InternVL (2024)** | OpenGBLam    | GPT-4o에 필적하는 공개형 VLM |
| **LMDeploy**        | 효율적 압축·서빙 툴킷 |                      |

---

### 6️⃣ **한국어 멀티모달 모델**

| 모델                                       | 개발             | 특징            |
| ---------------------------------------- | -------------- | ------------- |
| **HyperCLOVA X SEED Vision-Instruct-3B** | NAVER          | 한국어 시각지시 이해   |
| **ExaONE**                               | LG AI Research | 영어–한국어 이중 언어  |
| **SOLAR**                                | Upstage        | 번역 중심 모델      |
| **Phi / Gemma**                          | MS / Google    | 온디바이스 멀티모달 대응 |

---

### 7️⃣ **파운데이션 모델 활용 기술**

#### 💡 Fine-tuning

* 기존 모델을 특정 작업에 맞게 추가 학습
* 프롬프트보다 더 높은 품질·정확도
* 응답시간 단축, 토큰 절약

#### 💡 PEFT (Parameter Efficient Fine-Tuning)

* 모델 전체가 아닌 일부만 학습 (경량 학습)
* 종류:

  * **Prompt Tuning**: 학습 가능한 가상 프롬프트 추가
  * **Adapter Tuning**: 작은 모듈만 학습
* 장점: 지식 손실 없이 빠른 적응, 저비용

#### 💡 합성 데이터 (Synthetic Data)

* 실제 데이터 부족을 대체
* **지식증류(Knowledge Distillation)**

  * 큰 모델(Teacher)의 출력을 작은 모델(Student)에 전이
* **InstructPix2Pix (2023)**

  * 텍스트 지시로 이미지 편집 모델 학습
  * `{지시, 원본, 수정 후 이미지}` 형식 데이터

---

## 📚 핵심 요약

| 구분                            | 핵심 키워드                 |
| ----------------------------- | ---------------------- |
| **파운데이션 모델**                  | 대규모 자가학습, 전이학습, 범용성    |
| **CLIP / SigLIP**             | 이미지–텍스트 정합             |
| **LLaVA**                     | 비전–언어 통합, 시각 질의응답      |
| **SAM / DINO / Grounded-SAM** | 이미지·비디오 세그멘테이션         |
| **sVLM 계열**                   | 모바일·온디바이스용 초경량 모델      |
| **Fine-tuning / PEFT**        | 효율적 적응·개인화             |
| **한국어 모델**                    | HyperCLOVA X, ExaONE 등 |
| **합성 데이터**                    | 지식증류, 이미지 편집 데이터 생성    |
