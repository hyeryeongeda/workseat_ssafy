## 🧠 **4-1. LangChain 서비스 개발 요약**
---
### 1️⃣ **거대언어모델의 학습 구조**
#### 📘 **Pre-training (사전학습)**

* 방대한 인터넷 텍스트로 **언어 구조와 지식** 학습
* Self-supervised learning (다음 토큰 예측)

  > 예: “내일은 비가 ___ → 온다” 확률 최대화
* 문제점: 인간의 의도나 대화 맥락 반영이 부족

#### 📗 **Post-training (사후학습)**

* 목적: **모델이 인간의 의도에 맞게 응답하도록 조정**
* 주요 기법:

  * **Instruction-tuning**
  * **RLHF (Reinforcement Learning from Human Feedback)**
  * **DPO (Direct Preference Optimization)**
  * **RLVR (Reinforcement Learning with Verifiable Reward)**

---

### 2️⃣ **Instruction-tuning**

> 모델이 “사람의 지시문(instruction)”을 따르도록 파인튜닝하는 과정

* (지시문, 응답) 쌍 데이터로 학습
* 다양한 태스크에서 일반화 성능 향상
* 대표 평가: **MMLU (Massive Multitask Language Understanding)**
* 발전 모델: **Alpaca (LLaMA-7B + 52K instruction data)**
  → GPT-3로 합성한 지시문 데이터 활용
* 💡 **핵심 교훈:** 데이터 양보다 **품질이 더 중요**

**한계**

* 정답 데이터 수집 비용 큼
* 인간 선호와 모델 목적의 불일치

---

### 3️⃣ **RLHF (Reinforcement Learning from Human Feedback)**

> “인간의 선호를 반영한 강화학습”

**3단계 과정**
1️⃣ Instruction-tuning
2️⃣ 여러 답변 생성 후, 사람이 좋은 답에 높은 점수 부여
3️⃣ 보상모델(Reward Model)을 학습해 모델이 보상을 최대화하도록 강화학습

**특징**

* ChatGPT의 핵심 구조
* Instruction-tuning보다 인간 선호 반영이 뛰어남

**한계**

* 일관성 부족 (인간 평가 편향, Reward Hacking)
* “보기 좋은 말”을 택해 진실성이 떨어질 수 있음 → **Hallucination**

---

### 4️⃣ **DPO (Direct Preference Optimization)**

* RLHF의 강화학습 단계를 생략하고,
  **사람의 선호 데이터를 직접 활용해 모델을 최적화**
* 보상모델 불필요 → **학습 효율성↑, 안정성↑**
* 단점: 선호 데이터 품질에 크게 의존

---

### 5️⃣ **RLVR (Reinforcement Learning with Verifiable Reward)**

* 수학·논리 문제 등 **정답 검증 가능한 경우**
  → 보상을 “정답 여부”로 직접 평가
* 예시: **DeepSeek-R1**

---

## 🧩 **2. Retrieval-augmented LM (RAG)**

> “검색과 언어모델을 결합해, 외부 지식으로 강화된 LLM”

### 🔹 기본 구조

| 구성요소               | 설명                              |
| ------------------ | ------------------------------- |
| **Datastore**      | 비구조화 대규모 텍스트                    |
| **Index**          | 검색용 구조 (TF-IDF, BM25, 벡터 인덱스 등) |
| **Query**          | 검색 질의                           |
| **Language Model** | 검색 결과를 통합해 답변 생성                |

---

### 🔹 **Retriever 유형**

| 유형                   | 설명                 | 장점        | 단점         |
| -------------------- | ------------------ | --------- | ---------- |
| **Sparse Retriever** | TF-IDF, BM25 기반    | 빠름, 해석 용이 | 의미적 유사도 약함 |
| **Dense Retriever**  | 임베딩 기반 검색 (BERT 등) | 의미 이해 우수  | 연산 비용 큼    |

* **Bi-Encoder**: 빠르지만 정확도 낮음
* **Cross-Encoder**: 정확도 높지만 느림

---

### 🔹 **RAG 파이프라인**

1️⃣ 질의(Query) 추출
2️⃣ 문서 검색 (Retriever)
3️⃣ 검색 결과를 LLM 입력으로 전달해 답변 생성

**장점:** 최신 정보 활용 가능
**도전과제:**

* 컨텍스트 구성 난이도
* 검색 노이즈에 취약
* 모델 지식과 문맥 충돌
  → 대응: Noise Robustness, Grounding 강화, Counterfactual Robustness

---

## 🧠 **3. LLMs with Tool Usage (에이전트 기반 LLM)**

### 🔹 LLM Agent란?

> “환경을 인식하고, 도구를 활용해 목표를 수행하는 시스템”

**요건**

* Tool Use
* Reasoning & Planning
* Environment Representation
* Communication

---

### 🔹 Tool 유형

| 구분                     | 설명            | 예시                      |
| ---------------------- | ------------- | ----------------------- |
| **Physical Tools**     | 실제 환경 조작      | 로봇, IoT                 |
| **GUI Tools**          | 그래픽 인터페이스 조작  | 웹, Photoshop            |
| **Programmatic Tools** | API 호출, 코드 실행 | 계산기, DB, GitHub Copilot |

---

### 🔹 Tool Use 학습 방식

| 방식                         | 설명                        |
| -------------------------- | ------------------------- |
| **Imitation Learning**     | 인간의 행동 데이터 모방 (예: WebGPT) |
| **Supervised Fine-tuning** | API 호출 데이터로 지도학습          |
| **Reinforcement Learning** | 성공적 툴 사용 행동에 보상           |

---

## 🧩 **4. MCP (Model Context Protocol)**

> “모델과 외부 툴을 연결하는 표준 프로토콜”

* JSON-RPC 기반 통신 규격
* LLM이 다양한 툴(API, DB, 서비스)과 호환되도록 설계

**장점**

* 표준화된 구조 (tool/call 방식)
* 확장성, 재사용성, 투명성 향상
* 모델 간 호환성 확보

**예시 코드**

```python
@mcp.tool()
def add(a: int, b: int) -> int:
    """Add two numbers"""
    return a + b
```

---

## 🤖 **5. AI Agents & LangChain**

### 🔹 환경 표현 (Environment Representation)

* **Text 기반:** ALFWorld — 텍스트 시뮬레이션 환경
* **Image 기반:** Touchdown — 시각적 내비게이션
* **Web 기반:** WebArena — HTML 구조로 웹 환경 표현

---

### 🔹 Reasoning & Planning (추론 및 계획)

| 방식                  | 특징                                 |
| ------------------- | ---------------------------------- |
| **Local Planning**  | Step-by-step (ReAct, 2023)         |
| **Global Planning** | 전체 계획 경로 생성 (Plan-and-Solve, 2023) |
| **Self-Reflection** | 잘못된 답을 스스로 교정 (Reflexion, 2023)    |
| **Revisiting Plan** | 계획 수정/협력 (CoAct, 2024)             |

---

### 🔹 **LangChain**

> “LLM 기반 애플리케이션을 빠르게 개발할 수 있는 프레임워크”

**핵심 컴포넌트**

| 구성요소                | 설명                 |
| ------------------- | ------------------ |
| **Prompt Template** | 프롬프트 구조화           |
| **Chains**          | 단계적 워크플로우          |
| **Agents**          | 동적 도구 선택           |
| **Memory**          | 대화 히스토리 관리         |
| **Tools**           | 외부 API, DB, 계산기 연결 |

**활용 튜토리얼**

* [LangChain 공식 튜토리얼](https://python.langchain.com/docs/tutorials/)
* [RAG 예제](https://python.langchain.com/docs/tutorials/rag/)
* [Agent 예제](https://python.langchain.com/docs/tutorials/agents/)
* [MCP 어댑터](https://github.com/langchain-ai/langchain-mcp-adapters)

---

## 📚 **핵심 요약**

| 구분        | 키포인트                                         |
| --------- | -------------------------------------------- |
| 학습 구조     | Pre-training → Instruction-tuning → RLHF/DPO |
| RLHF 한계   | 편향·비일관성 → DPO, RLVR로 보완                      |
| RAG       | 외부지식 검색 + LLM 생성 결합                          |
| Tool Use  | LLM이 API, GUI, 로봇과 상호작용                      |
| MCP       | LLM–툴 통신 표준                                  |
| LangChain | Agent, RAG, MCP 통합 프레임워크                     |

---