
## 1ï¸âƒ£ ê±°ëŒ€ ì–¸ì–´ëª¨ë¸(LLM)ì˜ í•™ìŠµ íŒ¨ëŸ¬ë‹¤ì„

### ğŸ“˜ Pre-training vs Post-training

| êµ¬ë¶„    | Pre-training (ì‚¬ì „í•™ìŠµ) | Post-training (ì‚¬í›„í•™ìŠµ) |
| ----- | ------------------- | -------------------- |
| ëª©ì     | ì–¸ì–´ì™€ ì§€ì‹ í•™ìŠµ           | ì¸ê°„ì˜ ì˜ë„ì— ë§ëŠ” ë‹µë³€        |
| ë°ì´í„°   | ë°©ëŒ€í•œ ì¸í„°ë„· í…ìŠ¤íŠ¸         | (ì§€ì‹œë¬¸, ì‘ë‹µ) ìŒ ë°ì´í„°      |
| í•™ìŠµ ë°©ì‹ | Self-supervised     | Fine-tuning          |
| ì£¼ìš” ëª¨ë¸ | GPT, BERT ë“±         | ChatGPT, Claude ë“±    |

> ğŸ’¡ ì‚¬ì „í•™ìŠµëœ LLMì€ â€œì–¸ì–´ë¥¼ ë°°ìš´ ëª¨ë¸â€ì¼ ë¿, ì‚¬ìš©ìì˜ ì˜ë„ë¥¼ ì´í•´í•˜ì§€ ëª»í•œë‹¤.
> â†’ ì´ë¥¼ â€œì‚¬í›„í•™ìŠµ(Post-training)â€ìœ¼ë¡œ êµì •í•œë‹¤.

---

## 2ï¸âƒ£ Post-training í•µì‹¬ ê¸°ë²•

### ğŸ§© Instruction-tuning

* **ì§€ì‹œë¬¸ì„ ì´í•´í•˜ê³  ë”°ë¥´ë„ë¡** ëª¨ë¸ì„ í•™ìŠµ
* ë°ì´í„°: (Instruction, Response) ìŒ
* **ëª©í‘œ**: ì—¬ëŸ¬ íƒœìŠ¤í¬ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” â€œë²”ìš© ì ì‘í˜• ëª¨ë¸â€
* **ì˜ˆì‹œ**: Metaì˜ **Alpaca** (LLaMA + 52K GPT-3 ìƒì„± ë°ì´í„°)

> ğŸ’¬ ì¢‹ì€ ë°ì´í„°ê°€ ë§ì€ ë°ì´í„°ë³´ë‹¤ ì¤‘ìš”í•˜ë‹¤.
> (GPT-3 ê¸°ë°˜ ìë™ìƒì„± ë°ì´í„°ë¡œ ê³ ì„±ëŠ¥ ë‹¬ì„±)

#### ğŸ”¹ í•œê³„

* ë°ì´í„° ìˆ˜ì§‘ ë¹„ìš© ë†’ìŒ
* â€œì–¸ì–´ëª¨ë¸ ëª©í‘œ(LM Objective)â€ì™€ â€œì¸ê°„ì˜ ì„ í˜¸â€ ê°„ ë¶ˆì¼ì¹˜ ì¡´ì¬

---

### ğŸ§  RLHF (Reinforcement Learning from Human Feedback)

* ì¸ê°„ì˜ í”¼ë“œë°±ìœ¼ë¡œ **ëª¨ë¸ì´ ì„ í˜¸ë„ ë†’ì€ ë‹µì„ ì„ íƒí•˜ë„ë¡** í•™ìŠµ

#### í•™ìŠµ ë‹¨ê³„

1. Instruction-tuning ì‹¤ì‹œ
2. ì—¬ëŸ¬ ë‹µë³€ ìƒì„± â†’ ì‚¬ëŒ í‰ê°€
3. **Reward Model** í•™ìŠµ
4. ê°•í™”í•™ìŠµ(RL)ìœ¼ë¡œ ë³´ìƒ ìµœëŒ€í™”

#### ğŸ’¡ í•œê³„

* ì¸ê°„ íŒë‹¨ ë¶ˆì¼ì¹˜(Reward Hacking)
* Hallucination(ê·¼ê±° ì—†ëŠ” ë‹µë³€)
* ë³´ìƒëª¨ë¸ ìì²´ì˜ ë¶ˆì•ˆì •ì„±

---

### ğŸ§® DPO (Direct Preference Optimization)

* RLHFì—ì„œ **ê°•í™”í•™ìŠµ ë‹¨ê³„ ì œê±°**
* ì„ í˜¸ ë°ì´í„°ë¥¼ ì§ì ‘ í•™ìŠµí•˜ì—¬ ê°„ì†Œí™”ëœ íŒŒì´í”„ë¼ì¸

> âœ… ì¥ì : íš¨ìœ¨ì ì´ê³  ì•ˆì •ì 
> âš ï¸ ë‹¨ì : ë°ì´í„° í’ˆì§ˆì— í¬ê²Œ ì˜ì¡´

---

### ğŸ§© RLVR (Reinforcement Learning with Verifiable Reward)

* ìˆ˜í•™ ë¬¸ì œì²˜ëŸ¼ **ì •ë‹µì´ ëª…í™•í•œ ê²½ìš°**, ì •ë‹µ ì—¬ë¶€ë¥¼ ë³´ìƒìœ¼ë¡œ ì‚¬ìš©
* ì˜ˆì‹œ: **DeepSeek-R1**

---

## 3ï¸âƒ£ Retrieval-Augmented Language Model (RAG)

### ğŸ’¡ ê°œë…

> â€œì–¸ì–´ëª¨ë¸ì´ ì¶”ë¡  ì¤‘ì— ì™¸ë¶€ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ê²€ìƒ‰í•˜ì—¬ í™œìš©í•˜ëŠ” êµ¬ì¡°â€

êµ¬ì„± ìš”ì†Œ:

* **Datastore**: ë¬¸ì„œ ì €ì¥ì†Œ (ëŒ€ê·œëª¨ í…ìŠ¤íŠ¸)
* **Query**: ê²€ìƒ‰ ì§ˆì˜
* **Index**: ê²€ìƒ‰ìš© ì¸ë±ìŠ¤
* **Retriever + Generator**: ê²€ìƒ‰ + ìƒì„± ê²°í•©

---

### ğŸ§© Information Retrieval (ì •ë³´ê²€ìƒ‰)

* **ëª©í‘œ**: ì‚¬ìš©ìì˜ ì§ˆì˜(Query)ì— ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ì°¾ëŠ” ê²ƒ
* **ëŒ€í‘œ í™œìš©**: ì›¹ ê²€ìƒ‰, ì¶”ì²œ ì‹œìŠ¤í…œ, ì „ììƒê±°ë˜

#### ğŸ”¸ Retriever ì¢…ë¥˜

| ì¢…ë¥˜                   | ì„¤ëª…          | ì˜ˆì‹œ                                |
| -------------------- | ----------- | --------------------------------- |
| **Sparse Retriever** | ë‹¨ì–´ ì¼ì¹˜ ê¸°ë°˜ ê²€ìƒ‰ | TF-IDF, BM25                      |
| **Dense Retriever**  | ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰    | DPR, Contriever, OpenAI Embedding |

#### ğŸ’¡ ë¹„êµ

| êµ¬ë¶„  | Bi-Encoder    | Cross-Encoder |
| --- | ------------- | ------------- |
| êµ¬ì¡°  | ì¿¼ë¦¬/ë¬¸ì„œë¥¼ ë³„ë„ ì¸ì½”ë”© | ê²°í•© ì¸ì½”ë”©        |
| ì†ë„  | ë¹ ë¦„            | ëŠë¦¼            |
| ì •í™•ë„ | ë‚®ìŒ            | ë†’ìŒ            |

---

### âš™ï¸ RAGì˜ í•„ìš”ì„±

* LLMì€ ëª¨ë“  ì§€ì‹ì„ íŒŒë¼ë¯¸í„°ì— ì €ì¥í•  ìˆ˜ ì—†ìŒ
* ë“œë¬¸ ì •ë³´ë‚˜ ìµœì‹  ì •ë³´ëŠ” ì™¸ë¶€ DB ê²€ìƒ‰ìœ¼ë¡œ ë³´ì™„ ê°€ëŠ¥
* RAGì˜ DatabaseëŠ” **ì‰½ê²Œ ì—…ë°ì´íŠ¸ ê°€ëŠ¥**

---

### ğŸš§ RAGì˜ í•œê³„ì™€ ë„ì „ê³¼ì œ

1. **Context êµ¬ì„± ì–´ë ¤ì›€** (ê¸¸ì´ ì¡°ì ˆ, ì»¨í…ìŠ¤íŠ¸ í•œê³„)
2. **ê²€ìƒ‰ í’ˆì§ˆì— ì˜ì¡´** (ë…¸ì´ì¦ˆê°€ í™˜ê° ì¦ê°€)
3. **ì§€ì‹ ì¶©ëŒ ë¬¸ì œ** (LLMì˜ ë‚´ì¬ì§€ì‹ vs ê²€ìƒ‰ ê²°ê³¼)
4. **ì •ë³´ í†µí•©(Information Integration)**

   * ì—¬ëŸ¬ ë¬¸ì„œì—ì„œ ë‹µì„ ì¡°í•©í•´ì•¼ í•˜ëŠ” ë¬¸ì œ

> ğŸ’¡ í•´ê²° ë°©í–¥: Noise Robustness + Negative Rejection (ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  ë§í•˜ê¸°)

---

## 4ï¸âƒ£ LLM with Tool Usage â€” â€œì—ì´ì „íŠ¸ì˜ íƒ„ìƒâ€

### ğŸ§  LLM Agentë€?

* **í™˜ê²½ì„ ì¸ì§€í•˜ê³  í–‰ë™ì„ ìˆ˜í–‰í•˜ëŠ” ì‹œìŠ¤í…œ**
* LLMì´ ì¤‘ì‹¬ì´ ë˜ì–´ íˆ´ì„ í˜¸ì¶œí•˜ê±°ë‚˜ ë°ì´í„°ë¥¼ ê²€ìƒ‰í•˜ëŠ” **ì˜ì‚¬ê²°ì •í˜• AI**

#### êµ¬ì„±ìš”ì†Œ

| êµ¬ì„±          | ì„¤ëª…              |
| ----------- | --------------- |
| Controller  | ìš”ì²­ ì´í•´ ë° ê³„íš ìˆ˜ë¦½   |
| Tool Set    | API, ê³„ì‚°ê¸°, ê²€ìƒ‰ê¸° ë“± |
| Perceiver   | í™˜ê²½ í”¼ë“œë°± ìš”ì•½       |
| Environment | ë„êµ¬ê°€ ë™ì‘í•˜ëŠ” ì‹¤ì œ ê³µê°„  |
| Human       | ëª…ë ¹ê³¼ í”¼ë“œë°± ì œê³µ      |

---

### ğŸ›  Tool Usage

* LLMì´ ì™¸ë¶€ í”„ë¡œê·¸ë¨(API, SDK ë“±)ì„ í˜¸ì¶œí•˜ì—¬ ê¸°ëŠ¥ í™•ì¥
* **ë„êµ¬ ìœ í˜•**

  1. Physical (ë¡œë´‡, IoT)
  2. GUI (Photoshop, ì›¹ë¸Œë¼ìš°ì €)
  3. Program-based (API, DB, GitHub ë“±)

#### í•™ìŠµ ë°©ì‹

| ë°©ë²•                         | ì„¤ëª…               | ì˜ˆì‹œ              |
| -------------------------- | ---------------- | --------------- |
| **Imitation Learning**     | ì¸ê°„ì˜ ë„êµ¬ ì‚¬ìš©ì„ ëª¨ë°©    | WebGPT (OpenAI) |
| **Supervised Fine-tuning** | API í˜¸ì¶œ ë°ì´í„°ë¥¼ ì§€ë„í•™ìŠµ | ToolLLM         |
| **Reinforcement Learning** | ì‹¤ì œ ì‹¤í–‰ ê²°ê³¼ë¡œ ë³´ìƒ í•™ìŠµ  | GUI/ë¡œë´‡ ì—ì´ì „íŠ¸     |

---

## 5ï¸âƒ£ MCP (Model Context Protocol)

### ğŸ’¡ ë“±ì¥ ë°°ê²½

* ê° ëª¨ë¸ë§ˆë‹¤ íˆ´ í˜¸ì¶œ ë°©ì‹ì´ ë‹¬ë¼ **ë¹„í˜¸í™˜ì„±** ë°œìƒ
  â†’ í†µí•© í‘œì¤€ í•„ìš”

### âš™ï¸ MCP ì •ì˜

* LLMê³¼ ì™¸ë¶€ íˆ´ ê°„ **ìƒí˜¸ì‘ìš© í‘œì¤€ í”„ë¡œí† ì½œ**
* JSON-RPC ê¸°ë°˜
* ì£¼ìš” ê³„ì¸µ:

  * **Data Layer**: íˆ´ í˜¸ì¶œÂ·ì‘ë‹µ ì²˜ë¦¬
  * **Transport Layer**: í†µì‹ Â·ì¸ì¦ ê´€ë¦¬

> ğŸ§© MCPëŠ” í˜„ì¬ OpenAI, LangChain ë“±ì—ì„œ **í‘œì¤€ìœ¼ë¡œ ì±„íƒ ì¤‘**

#### ì˜ˆì‹œ í˜¸ì¶œ êµ¬ì¡°

```json
{
  "jsonrpc": "2.0",
  "id": 1,
  "method": "tools/call",
  "params": {
    "name": "get_weather",
    "arguments": {"location": "Seoul"}
  }
}
```

---

## 6ï¸âƒ£ AI Agents & LangChain

### ğŸ’¡ í•µì‹¬ í‚¤ì›Œë“œ

> LangChain = â€œLLM ê¸°ë°˜ ì„œë¹„ìŠ¤ ê°œë°œì„ ìœ„í•œ í‘œì¤€ í”„ë ˆì„ì›Œí¬â€

* ë‹¤ì–‘í•œ LLM(OpenAI, Anthropic, Google ë“±)ì„ í†µí•© ì§€ì›
* **Prompt, Memory, Tool, Agent** ë‹¨ìœ„ë¡œ êµ¬ì„±
* **LangGraph** ê¸°ë°˜ ì‹œê°ì  ì›Œí¬í”Œë¡œ ì„¤ê³„ ê°€ëŠ¥

---

### âš™ï¸ LangChain ì£¼ìš” ì»´í¬ë„ŒíŠ¸

| ì»´í¬ë„ŒíŠ¸               | ì„¤ëª…                 |
| ------------------ | ------------------ |
| **PromptTemplate** | êµ¬ì¡°í™”ëœ í”„ë¡¬í”„íŠ¸ ì •ì˜       |
| **Chains**         | ì—¬ëŸ¬ ë‹¨ê³„ë¥¼ ì—°ê²°í•œ ì›Œí¬í”Œë¡œ    |
| **Agents**         | íˆ´ ì„ íƒ ë° ì‹¤í–‰ ë¡œì§ í¬í•¨    |
| **Memory**         | ëŒ€í™” íˆìŠ¤í† ë¦¬ ì €ì¥         |
| **Tools**          | ì™¸ë¶€ API, DB, ê³„ì‚°ê¸° ì—°ë™ |

> ğŸ’¬ LangChainì€ RAG, Agent, MCP ëª¨ë‘ ì§€ì›í•˜ë©°
> AI ì„œë¹„ìŠ¤ êµ¬ì¶•ì˜ â€œí’€ìŠ¤íƒ í”„ë ˆì„ì›Œí¬â€ë¡œ ìë¦¬ì¡ì•˜ë‹¤.

---

### ğŸ“š LangChain íŠœí† ë¦¬ì–¼ ë§í¬

* ğŸ§± ê¸°ë³¸ íŠœí† ë¦¬ì–¼ â†’ [https://python.langchain.com/docs/tutorials/](https://python.langchain.com/docs/tutorials/)
* ğŸ“– RAG íŠœí† ë¦¬ì–¼ â†’ [https://python.langchain.com/docs/tutorials/rag/](https://python.langchain.com/docs/tutorials/rag/)
* ğŸ§© Agent íŠœí† ë¦¬ì–¼ â†’ [https://python.langchain.com/docs/tutorials/agents/](https://python.langchain.com/docs/tutorials/agents/)
* ğŸ›  MCP ì–´ëŒ‘í„° â†’ [https://github.com/langchain-ai/langchain-mcp-adapters](https://github.com/langchain-ai/langchain-mcp-adapters)

---

# âœ… í•µì‹¬ ìš”ì•½

| ì˜ì—­            | í•µì‹¬ ë‚´ìš©                                  |
| ------------- | -------------------------------------- |
| Post-Training | Instruction-tuning â†’ RLHF â†’ DPO â†’ RLVR |
| RAG           | ì™¸ë¶€ ì§€ì‹ ê²€ìƒ‰ + ìƒì„± ê²°í•©                       |
| Tool Use      | API, GUI, ë¡œë´‡ ë“± ì™¸ë¶€ ë„êµ¬ í™œìš©                |
| MCP           | íˆ´ í˜¸ì¶œ í‘œì¤€ í”„ë¡œí† ì½œ                           |
| Agent         | Reasoning + Planning + Action          |
| LangChain     | LLM + RAG + MCP + Tool í†µí•© í”„ë ˆì„ì›Œí¬        |


