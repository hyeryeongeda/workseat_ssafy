## ğŸ§  **4-1. LangChain ì„œë¹„ìŠ¤ ê°œë°œ ìš”ì•½**
---
### 1ï¸âƒ£ **ê±°ëŒ€ì–¸ì–´ëª¨ë¸ì˜ í•™ìŠµ êµ¬ì¡°**
#### ğŸ“˜ **Pre-training (ì‚¬ì „í•™ìŠµ)**

* ë°©ëŒ€í•œ ì¸í„°ë„· í…ìŠ¤íŠ¸ë¡œ **ì–¸ì–´ êµ¬ì¡°ì™€ ì§€ì‹** í•™ìŠµ
* Self-supervised learning (ë‹¤ìŒ í† í° ì˜ˆì¸¡)

  > ì˜ˆ: â€œë‚´ì¼ì€ ë¹„ê°€ ___ â†’ ì˜¨ë‹¤â€ í™•ë¥  ìµœëŒ€í™”
* ë¬¸ì œì : ì¸ê°„ì˜ ì˜ë„ë‚˜ ëŒ€í™” ë§¥ë½ ë°˜ì˜ì´ ë¶€ì¡±

#### ğŸ“— **Post-training (ì‚¬í›„í•™ìŠµ)**

* ëª©ì : **ëª¨ë¸ì´ ì¸ê°„ì˜ ì˜ë„ì— ë§ê²Œ ì‘ë‹µí•˜ë„ë¡ ì¡°ì •**
* ì£¼ìš” ê¸°ë²•:

  * **Instruction-tuning**
  * **RLHF (Reinforcement Learning from Human Feedback)**
  * **DPO (Direct Preference Optimization)**
  * **RLVR (Reinforcement Learning with Verifiable Reward)**

---

### 2ï¸âƒ£ **Instruction-tuning**

> ëª¨ë¸ì´ â€œì‚¬ëŒì˜ ì§€ì‹œë¬¸(instruction)â€ì„ ë”°ë¥´ë„ë¡ íŒŒì¸íŠœë‹í•˜ëŠ” ê³¼ì •

* (ì§€ì‹œë¬¸, ì‘ë‹µ) ìŒ ë°ì´í„°ë¡œ í•™ìŠµ
* ë‹¤ì–‘í•œ íƒœìŠ¤í¬ì—ì„œ ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ
* ëŒ€í‘œ í‰ê°€: **MMLU (Massive Multitask Language Understanding)**
* ë°œì „ ëª¨ë¸: **Alpaca (LLaMA-7B + 52K instruction data)**
  â†’ GPT-3ë¡œ í•©ì„±í•œ ì§€ì‹œë¬¸ ë°ì´í„° í™œìš©
* ğŸ’¡ **í•µì‹¬ êµí›ˆ:** ë°ì´í„° ì–‘ë³´ë‹¤ **í’ˆì§ˆì´ ë” ì¤‘ìš”**

**í•œê³„**

* ì •ë‹µ ë°ì´í„° ìˆ˜ì§‘ ë¹„ìš© í¼
* ì¸ê°„ ì„ í˜¸ì™€ ëª¨ë¸ ëª©ì ì˜ ë¶ˆì¼ì¹˜

---

### 3ï¸âƒ£ **RLHF (Reinforcement Learning from Human Feedback)**

> â€œì¸ê°„ì˜ ì„ í˜¸ë¥¼ ë°˜ì˜í•œ ê°•í™”í•™ìŠµâ€

**3ë‹¨ê³„ ê³¼ì •**
1ï¸âƒ£ Instruction-tuning
2ï¸âƒ£ ì—¬ëŸ¬ ë‹µë³€ ìƒì„± í›„, ì‚¬ëŒì´ ì¢‹ì€ ë‹µì— ë†’ì€ ì ìˆ˜ ë¶€ì—¬
3ï¸âƒ£ ë³´ìƒëª¨ë¸(Reward Model)ì„ í•™ìŠµí•´ ëª¨ë¸ì´ ë³´ìƒì„ ìµœëŒ€í™”í•˜ë„ë¡ ê°•í™”í•™ìŠµ

**íŠ¹ì§•**

* ChatGPTì˜ í•µì‹¬ êµ¬ì¡°
* Instruction-tuningë³´ë‹¤ ì¸ê°„ ì„ í˜¸ ë°˜ì˜ì´ ë›°ì–´ë‚¨

**í•œê³„**

* ì¼ê´€ì„± ë¶€ì¡± (ì¸ê°„ í‰ê°€ í¸í–¥, Reward Hacking)
* â€œë³´ê¸° ì¢‹ì€ ë§â€ì„ íƒí•´ ì§„ì‹¤ì„±ì´ ë–¨ì–´ì§ˆ ìˆ˜ ìˆìŒ â†’ **Hallucination**

---

### 4ï¸âƒ£ **DPO (Direct Preference Optimization)**

* RLHFì˜ ê°•í™”í•™ìŠµ ë‹¨ê³„ë¥¼ ìƒëµí•˜ê³ ,
  **ì‚¬ëŒì˜ ì„ í˜¸ ë°ì´í„°ë¥¼ ì§ì ‘ í™œìš©í•´ ëª¨ë¸ì„ ìµœì í™”**
* ë³´ìƒëª¨ë¸ ë¶ˆí•„ìš” â†’ **í•™ìŠµ íš¨ìœ¨ì„±â†‘, ì•ˆì •ì„±â†‘**
* ë‹¨ì : ì„ í˜¸ ë°ì´í„° í’ˆì§ˆì— í¬ê²Œ ì˜ì¡´

---

### 5ï¸âƒ£ **RLVR (Reinforcement Learning with Verifiable Reward)**

* ìˆ˜í•™Â·ë…¼ë¦¬ ë¬¸ì œ ë“± **ì •ë‹µ ê²€ì¦ ê°€ëŠ¥í•œ ê²½ìš°**
  â†’ ë³´ìƒì„ â€œì •ë‹µ ì—¬ë¶€â€ë¡œ ì§ì ‘ í‰ê°€
* ì˜ˆì‹œ: **DeepSeek-R1**

---

## ğŸ§© **2. Retrieval-augmented LM (RAG)**

> â€œê²€ìƒ‰ê³¼ ì–¸ì–´ëª¨ë¸ì„ ê²°í•©í•´, ì™¸ë¶€ ì§€ì‹ìœ¼ë¡œ ê°•í™”ëœ LLMâ€

### ğŸ”¹ ê¸°ë³¸ êµ¬ì¡°

| êµ¬ì„±ìš”ì†Œ               | ì„¤ëª…                              |
| ------------------ | ------------------------------- |
| **Datastore**      | ë¹„êµ¬ì¡°í™” ëŒ€ê·œëª¨ í…ìŠ¤íŠ¸                    |
| **Index**          | ê²€ìƒ‰ìš© êµ¬ì¡° (TF-IDF, BM25, ë²¡í„° ì¸ë±ìŠ¤ ë“±) |
| **Query**          | ê²€ìƒ‰ ì§ˆì˜                           |
| **Language Model** | ê²€ìƒ‰ ê²°ê³¼ë¥¼ í†µí•©í•´ ë‹µë³€ ìƒì„±                |

---

### ğŸ”¹ **Retriever ìœ í˜•**

| ìœ í˜•                   | ì„¤ëª…                 | ì¥ì         | ë‹¨ì          |
| -------------------- | ------------------ | --------- | ---------- |
| **Sparse Retriever** | TF-IDF, BM25 ê¸°ë°˜    | ë¹ ë¦„, í•´ì„ ìš©ì´ | ì˜ë¯¸ì  ìœ ì‚¬ë„ ì•½í•¨ |
| **Dense Retriever**  | ì„ë² ë”© ê¸°ë°˜ ê²€ìƒ‰ (BERT ë“±) | ì˜ë¯¸ ì´í•´ ìš°ìˆ˜  | ì—°ì‚° ë¹„ìš© í¼    |

* **Bi-Encoder**: ë¹ ë¥´ì§€ë§Œ ì •í™•ë„ ë‚®ìŒ
* **Cross-Encoder**: ì •í™•ë„ ë†’ì§€ë§Œ ëŠë¦¼

---

### ğŸ”¹ **RAG íŒŒì´í”„ë¼ì¸**

1ï¸âƒ£ ì§ˆì˜(Query) ì¶”ì¶œ
2ï¸âƒ£ ë¬¸ì„œ ê²€ìƒ‰ (Retriever)
3ï¸âƒ£ ê²€ìƒ‰ ê²°ê³¼ë¥¼ LLM ì…ë ¥ìœ¼ë¡œ ì „ë‹¬í•´ ë‹µë³€ ìƒì„±

**ì¥ì :** ìµœì‹  ì •ë³´ í™œìš© ê°€ëŠ¥
**ë„ì „ê³¼ì œ:**

* ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ë‚œì´ë„
* ê²€ìƒ‰ ë…¸ì´ì¦ˆì— ì·¨ì•½
* ëª¨ë¸ ì§€ì‹ê³¼ ë¬¸ë§¥ ì¶©ëŒ
  â†’ ëŒ€ì‘: Noise Robustness, Grounding ê°•í™”, Counterfactual Robustness

---

## ğŸ§  **3. LLMs with Tool Usage (ì—ì´ì „íŠ¸ ê¸°ë°˜ LLM)**

### ğŸ”¹ LLM Agentë€?

> â€œí™˜ê²½ì„ ì¸ì‹í•˜ê³ , ë„êµ¬ë¥¼ í™œìš©í•´ ëª©í‘œë¥¼ ìˆ˜í–‰í•˜ëŠ” ì‹œìŠ¤í…œâ€

**ìš”ê±´**

* Tool Use
* Reasoning & Planning
* Environment Representation
* Communication

---

### ğŸ”¹ Tool ìœ í˜•

| êµ¬ë¶„                     | ì„¤ëª…            | ì˜ˆì‹œ                      |
| ---------------------- | ------------- | ----------------------- |
| **Physical Tools**     | ì‹¤ì œ í™˜ê²½ ì¡°ì‘      | ë¡œë´‡, IoT                 |
| **GUI Tools**          | ê·¸ë˜í”½ ì¸í„°í˜ì´ìŠ¤ ì¡°ì‘  | ì›¹, Photoshop            |
| **Programmatic Tools** | API í˜¸ì¶œ, ì½”ë“œ ì‹¤í–‰ | ê³„ì‚°ê¸°, DB, GitHub Copilot |

---

### ğŸ”¹ Tool Use í•™ìŠµ ë°©ì‹

| ë°©ì‹                         | ì„¤ëª…                        |
| -------------------------- | ------------------------- |
| **Imitation Learning**     | ì¸ê°„ì˜ í–‰ë™ ë°ì´í„° ëª¨ë°© (ì˜ˆ: WebGPT) |
| **Supervised Fine-tuning** | API í˜¸ì¶œ ë°ì´í„°ë¡œ ì§€ë„í•™ìŠµ          |
| **Reinforcement Learning** | ì„±ê³µì  íˆ´ ì‚¬ìš© í–‰ë™ì— ë³´ìƒ           |

---

## ğŸ§© **4. MCP (Model Context Protocol)**

> â€œëª¨ë¸ê³¼ ì™¸ë¶€ íˆ´ì„ ì—°ê²°í•˜ëŠ” í‘œì¤€ í”„ë¡œí† ì½œâ€

* JSON-RPC ê¸°ë°˜ í†µì‹  ê·œê²©
* LLMì´ ë‹¤ì–‘í•œ íˆ´(API, DB, ì„œë¹„ìŠ¤)ê³¼ í˜¸í™˜ë˜ë„ë¡ ì„¤ê³„

**ì¥ì **

* í‘œì¤€í™”ëœ êµ¬ì¡° (tool/call ë°©ì‹)
* í™•ì¥ì„±, ì¬ì‚¬ìš©ì„±, íˆ¬ëª…ì„± í–¥ìƒ
* ëª¨ë¸ ê°„ í˜¸í™˜ì„± í™•ë³´

**ì˜ˆì‹œ ì½”ë“œ**

```python
@mcp.tool()
def add(a: int, b: int) -> int:
    """Add two numbers"""
    return a + b
```

---

## ğŸ¤– **5. AI Agents & LangChain**

### ğŸ”¹ í™˜ê²½ í‘œí˜„ (Environment Representation)

* **Text ê¸°ë°˜:** ALFWorld â€” í…ìŠ¤íŠ¸ ì‹œë®¬ë ˆì´ì…˜ í™˜ê²½
* **Image ê¸°ë°˜:** Touchdown â€” ì‹œê°ì  ë‚´ë¹„ê²Œì´ì…˜
* **Web ê¸°ë°˜:** WebArena â€” HTML êµ¬ì¡°ë¡œ ì›¹ í™˜ê²½ í‘œí˜„

---

### ğŸ”¹ Reasoning & Planning (ì¶”ë¡  ë° ê³„íš)

| ë°©ì‹                  | íŠ¹ì§•                                 |
| ------------------- | ---------------------------------- |
| **Local Planning**  | Step-by-step (ReAct, 2023)         |
| **Global Planning** | ì „ì²´ ê³„íš ê²½ë¡œ ìƒì„± (Plan-and-Solve, 2023) |
| **Self-Reflection** | ì˜ëª»ëœ ë‹µì„ ìŠ¤ìŠ¤ë¡œ êµì • (Reflexion, 2023)    |
| **Revisiting Plan** | ê³„íš ìˆ˜ì •/í˜‘ë ¥ (CoAct, 2024)             |

---

### ğŸ”¹ **LangChain**

> â€œLLM ê¸°ë°˜ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ë¹ ë¥´ê²Œ ê°œë°œí•  ìˆ˜ ìˆëŠ” í”„ë ˆì„ì›Œí¬â€

**í•µì‹¬ ì»´í¬ë„ŒíŠ¸**

| êµ¬ì„±ìš”ì†Œ                | ì„¤ëª…                 |
| ------------------- | ------------------ |
| **Prompt Template** | í”„ë¡¬í”„íŠ¸ êµ¬ì¡°í™”           |
| **Chains**          | ë‹¨ê³„ì  ì›Œí¬í”Œë¡œìš°          |
| **Agents**          | ë™ì  ë„êµ¬ ì„ íƒ           |
| **Memory**          | ëŒ€í™” íˆìŠ¤í† ë¦¬ ê´€ë¦¬         |
| **Tools**           | ì™¸ë¶€ API, DB, ê³„ì‚°ê¸° ì—°ê²° |

**í™œìš© íŠœí† ë¦¬ì–¼**

* [LangChain ê³µì‹ íŠœí† ë¦¬ì–¼](https://python.langchain.com/docs/tutorials/)
* [RAG ì˜ˆì œ](https://python.langchain.com/docs/tutorials/rag/)
* [Agent ì˜ˆì œ](https://python.langchain.com/docs/tutorials/agents/)
* [MCP ì–´ëŒ‘í„°](https://github.com/langchain-ai/langchain-mcp-adapters)

---

## ğŸ“š **í•µì‹¬ ìš”ì•½**

| êµ¬ë¶„        | í‚¤í¬ì¸íŠ¸                                         |
| --------- | -------------------------------------------- |
| í•™ìŠµ êµ¬ì¡°     | Pre-training â†’ Instruction-tuning â†’ RLHF/DPO |
| RLHF í•œê³„   | í¸í–¥Â·ë¹„ì¼ê´€ì„± â†’ DPO, RLVRë¡œ ë³´ì™„                      |
| RAG       | ì™¸ë¶€ì§€ì‹ ê²€ìƒ‰ + LLM ìƒì„± ê²°í•©                          |
| Tool Use  | LLMì´ API, GUI, ë¡œë´‡ê³¼ ìƒí˜¸ì‘ìš©                      |
| MCP       | LLMâ€“íˆ´ í†µì‹  í‘œì¤€                                  |
| LangChain | Agent, RAG, MCP í†µí•© í”„ë ˆì„ì›Œí¬                     |

---