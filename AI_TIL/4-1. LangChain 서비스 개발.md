
## 1️⃣ 거대 언어모델(LLM)의 학습 패러다임

### 📘 Pre-training vs Post-training

| 구분    | Pre-training (사전학습) | Post-training (사후학습) |
| ----- | ------------------- | -------------------- |
| 목적    | 언어와 지식 학습           | 인간의 의도에 맞는 답변        |
| 데이터   | 방대한 인터넷 텍스트         | (지시문, 응답) 쌍 데이터      |
| 학습 방식 | Self-supervised     | Fine-tuning          |
| 주요 모델 | GPT, BERT 등         | ChatGPT, Claude 등    |

> 💡 사전학습된 LLM은 “언어를 배운 모델”일 뿐, 사용자의 의도를 이해하지 못한다.
> → 이를 “사후학습(Post-training)”으로 교정한다.

---

## 2️⃣ Post-training 핵심 기법

### 🧩 Instruction-tuning

* **지시문을 이해하고 따르도록** 모델을 학습
* 데이터: (Instruction, Response) 쌍
* **목표**: 여러 태스크를 수행할 수 있는 “범용 적응형 모델”
* **예시**: Meta의 **Alpaca** (LLaMA + 52K GPT-3 생성 데이터)

> 💬 좋은 데이터가 많은 데이터보다 중요하다.
> (GPT-3 기반 자동생성 데이터로 고성능 달성)

#### 🔹 한계

* 데이터 수집 비용 높음
* “언어모델 목표(LM Objective)”와 “인간의 선호” 간 불일치 존재

---

### 🧠 RLHF (Reinforcement Learning from Human Feedback)

* 인간의 피드백으로 **모델이 선호도 높은 답을 선택하도록** 학습

#### 학습 단계

1. Instruction-tuning 실시
2. 여러 답변 생성 → 사람 평가
3. **Reward Model** 학습
4. 강화학습(RL)으로 보상 최대화

#### 💡 한계

* 인간 판단 불일치(Reward Hacking)
* Hallucination(근거 없는 답변)
* 보상모델 자체의 불안정성

---

### 🧮 DPO (Direct Preference Optimization)

* RLHF에서 **강화학습 단계 제거**
* 선호 데이터를 직접 학습하여 간소화된 파이프라인

> ✅ 장점: 효율적이고 안정적
> ⚠️ 단점: 데이터 품질에 크게 의존

---

### 🧩 RLVR (Reinforcement Learning with Verifiable Reward)

* 수학 문제처럼 **정답이 명확한 경우**, 정답 여부를 보상으로 사용
* 예시: **DeepSeek-R1**

---

## 3️⃣ Retrieval-Augmented Language Model (RAG)

### 💡 개념

> “언어모델이 추론 중에 외부 데이터베이스를 검색하여 활용하는 구조”

구성 요소:

* **Datastore**: 문서 저장소 (대규모 텍스트)
* **Query**: 검색 질의
* **Index**: 검색용 인덱스
* **Retriever + Generator**: 검색 + 생성 결합

---

### 🧩 Information Retrieval (정보검색)

* **목표**: 사용자의 질의(Query)에 관련된 문서를 찾는 것
* **대표 활용**: 웹 검색, 추천 시스템, 전자상거래

#### 🔸 Retriever 종류

| 종류                   | 설명          | 예시                                |
| -------------------- | ----------- | --------------------------------- |
| **Sparse Retriever** | 단어 일치 기반 검색 | TF-IDF, BM25                      |
| **Dense Retriever**  | 의미 기반 검색    | DPR, Contriever, OpenAI Embedding |

#### 💡 비교

| 구분  | Bi-Encoder    | Cross-Encoder |
| --- | ------------- | ------------- |
| 구조  | 쿼리/문서를 별도 인코딩 | 결합 인코딩        |
| 속도  | 빠름            | 느림            |
| 정확도 | 낮음            | 높음            |

---

### ⚙️ RAG의 필요성

* LLM은 모든 지식을 파라미터에 저장할 수 없음
* 드문 정보나 최신 정보는 외부 DB 검색으로 보완 가능
* RAG의 Database는 **쉽게 업데이트 가능**

---

### 🚧 RAG의 한계와 도전과제

1. **Context 구성 어려움** (길이 조절, 컨텍스트 한계)
2. **검색 품질에 의존** (노이즈가 환각 증가)
3. **지식 충돌 문제** (LLM의 내재지식 vs 검색 결과)
4. **정보 통합(Information Integration)**

   * 여러 문서에서 답을 조합해야 하는 문제

> 💡 해결 방향: Noise Robustness + Negative Rejection (모르면 모른다고 말하기)

---

## 4️⃣ LLM with Tool Usage — “에이전트의 탄생”

### 🧠 LLM Agent란?

* **환경을 인지하고 행동을 수행하는 시스템**
* LLM이 중심이 되어 툴을 호출하거나 데이터를 검색하는 **의사결정형 AI**

#### 구성요소

| 구성          | 설명              |
| ----------- | --------------- |
| Controller  | 요청 이해 및 계획 수립   |
| Tool Set    | API, 계산기, 검색기 등 |
| Perceiver   | 환경 피드백 요약       |
| Environment | 도구가 동작하는 실제 공간  |
| Human       | 명령과 피드백 제공      |

---

### 🛠 Tool Usage

* LLM이 외부 프로그램(API, SDK 등)을 호출하여 기능 확장
* **도구 유형**

  1. Physical (로봇, IoT)
  2. GUI (Photoshop, 웹브라우저)
  3. Program-based (API, DB, GitHub 등)

#### 학습 방식

| 방법                         | 설명               | 예시              |
| -------------------------- | ---------------- | --------------- |
| **Imitation Learning**     | 인간의 도구 사용을 모방    | WebGPT (OpenAI) |
| **Supervised Fine-tuning** | API 호출 데이터를 지도학습 | ToolLLM         |
| **Reinforcement Learning** | 실제 실행 결과로 보상 학습  | GUI/로봇 에이전트     |

---

## 5️⃣ MCP (Model Context Protocol)

### 💡 등장 배경

* 각 모델마다 툴 호출 방식이 달라 **비호환성** 발생
  → 통합 표준 필요

### ⚙️ MCP 정의

* LLM과 외부 툴 간 **상호작용 표준 프로토콜**
* JSON-RPC 기반
* 주요 계층:

  * **Data Layer**: 툴 호출·응답 처리
  * **Transport Layer**: 통신·인증 관리

> 🧩 MCP는 현재 OpenAI, LangChain 등에서 **표준으로 채택 중**

#### 예시 호출 구조

```json
{
  "jsonrpc": "2.0",
  "id": 1,
  "method": "tools/call",
  "params": {
    "name": "get_weather",
    "arguments": {"location": "Seoul"}
  }
}
```

---

## 6️⃣ AI Agents & LangChain

### 💡 핵심 키워드

> LangChain = “LLM 기반 서비스 개발을 위한 표준 프레임워크”

* 다양한 LLM(OpenAI, Anthropic, Google 등)을 통합 지원
* **Prompt, Memory, Tool, Agent** 단위로 구성
* **LangGraph** 기반 시각적 워크플로 설계 가능

---

### ⚙️ LangChain 주요 컴포넌트

| 컴포넌트               | 설명                 |
| ------------------ | ------------------ |
| **PromptTemplate** | 구조화된 프롬프트 정의       |
| **Chains**         | 여러 단계를 연결한 워크플로    |
| **Agents**         | 툴 선택 및 실행 로직 포함    |
| **Memory**         | 대화 히스토리 저장         |
| **Tools**          | 외부 API, DB, 계산기 연동 |

> 💬 LangChain은 RAG, Agent, MCP 모두 지원하며
> AI 서비스 구축의 “풀스택 프레임워크”로 자리잡았다.

---

### 📚 LangChain 튜토리얼 링크

* 🧱 기본 튜토리얼 → [https://python.langchain.com/docs/tutorials/](https://python.langchain.com/docs/tutorials/)
* 📖 RAG 튜토리얼 → [https://python.langchain.com/docs/tutorials/rag/](https://python.langchain.com/docs/tutorials/rag/)
* 🧩 Agent 튜토리얼 → [https://python.langchain.com/docs/tutorials/agents/](https://python.langchain.com/docs/tutorials/agents/)
* 🛠 MCP 어댑터 → [https://github.com/langchain-ai/langchain-mcp-adapters](https://github.com/langchain-ai/langchain-mcp-adapters)

---

# ✅ 핵심 요약

| 영역            | 핵심 내용                                  |
| ------------- | -------------------------------------- |
| Post-Training | Instruction-tuning → RLHF → DPO → RLVR |
| RAG           | 외부 지식 검색 + 생성 결합                       |
| Tool Use      | API, GUI, 로봇 등 외부 도구 활용                |
| MCP           | 툴 호출 표준 프로토콜                           |
| Agent         | Reasoning + Planning + Action          |
| LangChain     | LLM + RAG + MCP + Tool 통합 프레임워크        |


