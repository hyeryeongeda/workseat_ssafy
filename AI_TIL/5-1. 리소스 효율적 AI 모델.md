## ⚙️ **5-1. 리소스 효율적 AI 모델**
---
### 1️⃣ **컴퓨터의 수 체계와 연산 복잡도**

#### 🔹 **이진수와 자료 단위**

* 2진수 → 0과 1로만 구성
* 1 byte = 8 bits
* 1 nibble = 4 bits
* 2^10 = 1K (약 1천), 2^30 = 1G (약 10억)

#### 🔹 **부호 있는 수의 표현**

| 방식                              | 특징                            |
| ------------------------------- | ----------------------------- |
| **부호-크기 표현 (Signed Magnitude)** | +0, -0 존재 → 연산 불편             |
| **2의 보수 표현 (Two’s Complement)** | 가장 널리 사용. 부호비트=MSB (0:+, 1:-) |

> **N-bit 2의 보수 범위:** [−2^(N−1), 2^(N−1)−1]

#### 🔹 **고정소수점 수 체계 (Fixed-Point)**

* 정수부 + 소수부로 나누어 표현
* `K bit(정수부) + M bit(소수부)`
* 실제 값 = 저장값 × 2^(−M)
* 덧셈·뺄셈은 동일하지만, 곱셈·나눗셈 시 scaling factor 주의

💡 **단점:** 큰 수/작은 수 표현 시 비트 낭비

---

### 2️⃣ **부동소수점 수 체계 (Floating-Point)**

#### 🔹 기본 표현식

[
F = (-1)^S × M × B^E
]

* S: 부호
* M: 가수 (mantissa)
* E: 지수 (exponent)

#### 🔹 **정규화(Normalization)**

* 항상 `1.xxx × 2^E` 형태로 표현 (hidden bit 사용)
* 가수 앞자리가 1로 고정 → 효율적 저장

#### 🔹 **Bias (바이어스 지수)**

* 음수 지수도 양수로 저장하기 위한 보정
* 예: e=8bit → bias=127
* 실제 지수 = 저장값 − 127

#### 🔹 **IEEE 754 표준**

| 포맷                  | 총 비트  | 지수 | 가수 | 대표 자료형 |
| ------------------- | ----- | -- | -- | ------ |
| **Single (float)**  | 32bit | 8  | 23 | float  |
| **Double (double)** | 64bit | 11 | 52 | double |

> E=0, f=0 → 0
> E=255, f≠0 → NaN
> E=255, f=0 → ±∞

---

### 3️⃣ **모델 경량화의 필요성**

* LLM·딥러닝 모델 파라미터 수 증가 → 메모리/연산 부담
* GPU·NPU 리소스 부족 문제
* 모바일·엣지 환경에서는 필수 기술

**결과:**
모델의 크기를 줄이면서 성능을 최대한 유지하는 기술이 요구됨

---

## 🧩 **2. 모델 경량화 주요 기법**

---

### 🔹 **1) Quantization (양자화)**

> **모델 파라미터를 낮은 정밀도의 비트로 표현**

| 구분                                    | 설명                          |
| ------------------------------------- | --------------------------- |
| **QAT (Quantization Aware Training)** | 학습 중 양자화 시뮬레이션. 정확도↑, 비용↑   |
| **PTQ (Post Training Quantization)**  | 학습 후 양자화. 빠르고 간단, 정확도 손실 가능 |

#### ⚙️ 세부 분류

| 종류                        | 특징                           |
| ------------------------- | ---------------------------- |
| **Weight-Activation 양자화** | 가중치·활성값 모두 정수화 → 속도↑, 정확도↓   |
| **Weight-only 양자화**       | 가중치만 정수화 → 속도 이득↓, 정확도 손실 적음 |

#### 🔸 대칭(Symmetric) vs 비대칭(Asymmetric)

| 유형      | 설명                 | 장점     | 단점         |
| ------- | ------------------ | ------ | ---------- |
| **대칭**  | 0 중심, scale만 사용    | 연산 단순  | 분포 비대칭시 낭비 |
| **비대칭** | scale + zero-point | 정확한 표현 | 연산 복잡      |

#### 🔸 부동소수점 양자화 (FP Formats)

* **BF16:** FP32의 truncated 버전 (구글 TPU)
* **TF32:** FP32 범위, FP16 정밀도
* **FP8 (E5M2, E4M3):** 초저정밀 학습용

> ⚡ Mixed-Precision Quantization: 레이어별 정밀도 조정으로 성능 균형 유지

---

### 🔹 **2) Pruning (가지치기)**

> 불필요한 가중치(Weight)를 제거하여 모델을 희소화(Sparse)하는 기법

| 방식                  | 설명                               |
| ------------------- | -------------------------------- |
| **Magnitude-based** | 절댓값 작은 weight 제거                 |
| **Unstructured**    | 무작위 제거, 압축률 높음 but GPU 비효율적      |
| **Structured**      | 채널·레이어 단위 제거, 속도 향상 but 성능 저하 위험 |

💡 **효과:**

* 저장공간↓
* 속도 향상(전용 하드웨어 필요)
* CNN: 80% pruning 가능 (정확도 1~2% 손실)
* LLM: 50% 수준이 한계

---

### 🔹 **3) Knowledge Distillation (지식 증류)**

> 큰 모델(Teacher)의 지식을 작은 모델(Student)로 전이

* **Loss 구성:**
  `Total = Student Loss + Distillation Loss`
* **TAKD (Teacher Assistant KD):** 중간 모델(조교)로 성능 간극 완화
* **LLM Distillation:** 대형 LLM → 소형 LLM 전이 (예: GPT → Phi-2, MiniCPM 등)

---

### 🔹 **4) LoRA / QLoRA (저비용 파인튜닝)**

> **원본 모델을 건드리지 않고 소규모 파라미터만 학습**

| 방식                             | 특징                                  |
| ------------------------------ | ----------------------------------- |
| **LoRA (Low-Rank Adaptation)** | 특정 weight matrix에 저차원(rank) 업데이트 추가 |
| **QLoRA**                      | 4bit 양자화된 모델에 LoRA 적용 (저비용·고성능 조합)  |

#### ✅ 장점

* Full fine-tuning 대비 메모리 90% 절감
* GPU VRAM 적게 소모
* 원본 모델 지식 손실 거의 없음

#### ⚙️ 관련 라이브러리

* **Unsloth, Axolotl:** QLoRA 자동화 프레임워크
* **매개변수:** rank, alpha, target_modules

---

### 🔹 **5) Adapter / Prompt Tuning**

| 기법                | 설명                                |
| ----------------- | --------------------------------- |
| **Adapter Layer** | 모델 내부에 작은 MLP 레이어 삽입, 해당 부분만 학습   |
| **Prompt Tuning** | 입력 앞에 학습 가능한 토큰 추가 (pseudo-token) |

💡 Prompt Tuning → 가장 가볍고 배포 쉬움
하지만 표현력 한계 있음

---

## 📚 **정리 요약**

| 구분                          | 기술          | 주요 목적      |
| --------------------------- | ----------- | ---------- |
| **Quantization**            | 정밀도 축소      | 메모리·연산 절감  |
| **Pruning**                 | 불필요 가중치 제거  | 희소화, 속도 향상 |
| **Distillation**            | 지식 전이       | 작은 모델 고성능화 |
| **LoRA / QLoRA**            | 부분 파라미터 학습  | 효율적 도메인 튜닝 |
| **Prompt / Adapter Tuning** | 입력·내부 구조 조정 | 서비스 특화     |

---