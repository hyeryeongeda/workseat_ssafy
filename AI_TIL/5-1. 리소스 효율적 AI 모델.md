## 1ï¸âƒ£ í•™ìŠµ ëª©í‘œ

* ì»´í“¨í„°ê°€ ìˆ˜ë¥¼ ì´í•´í•˜ê³  ì²˜ë¦¬í•˜ëŠ” **ì´ì§„ìˆ˜ ì²´ê³„** ì´í•´
* **ê³ ì •ì†Œìˆ˜ì  / ë¶€ë™ì†Œìˆ˜ì ** êµ¬ì¡°ì˜ ì°¨ì´ì™€ ì—°ì‚° ë³µì¡ë„ ì´í•´
* **ëª¨ë¸ ê²½ëŸ‰í™”(Quantization, Pruning, Distillation)** ê¸°ë²•ì˜ ì›ë¦¬ì™€ í™œìš©
* **íŒŒë¼ë¯¸í„° íš¨ìœ¨ì  íŒŒì¸íŠœë‹ (PEFT)** ê°œë… ë° **LoRA / QLoRA** ì˜ ì‘ë™ ì›ë¦¬ ì´í•´

---

## 2ï¸âƒ£ ì»´í“¨í„°ì˜ ìˆ˜ ì²´ê³„ (Binary Number System)

### âš™ï¸ 2ì§„ìˆ˜ ê¸°ë³¸ êµ¬ì¡°

* 2ì§„ìˆ˜ëŠ” 0ê³¼ 1ë§Œì„ ì‚¬ìš©í•˜ë©°, ê° ìë¦¿ìˆ˜ëŠ” 2ì˜ ì§€ìˆ˜ë¥¼ ë‚˜íƒ€ëƒ„
* `2^10 â‰ˆ 1,000`, `2^20 â‰ˆ 1,000,000`, `2^30 â‰ˆ 1,000,000,000`

---

### ğŸ§® ê³ ì •ì†Œìˆ˜ì  (Fixed-Point Number)

* N-bit ë‚´ì—ì„œ **ì •ìˆ˜ë¶€ + ì†Œìˆ˜ë¶€**ë¥¼ í•¨ê»˜ í‘œí˜„
* ì†Œìˆ˜ì  ìœ„ì¹˜ëŠ” ê³ ì •ë˜ì§€ ì•Šê³  â€œì‚¬ìš©ìê°€ ì§€ì •â€ (scaling factor ì‚¬ìš©)
* ê³„ì‚° ì˜ˆ:

  ```
  (4,4) 8bit ê³ ì •ì†Œìˆ˜ì  â†’ 4bit ì •ìˆ˜, 4bit ì†Œìˆ˜
  3.5 + 1.25 = 4.75
  ì‹¤ì œ ì—°ì‚°: (3.5Ã—16 + 1.25Ã—16) / 16 = 76/16 = 4.75
  ```
* ğŸ’¡ ë‹¨ì : í° ìˆ˜/ì‘ì€ ìˆ˜ í‘œí˜„ ì‹œ ë¹„íŠ¸ ë‚­ë¹„ í¼ â†’ ë¶€ë™ì†Œìˆ˜ì  ë“±ì¥

---

### ğŸ“˜ ë¶€ë™ì†Œìˆ˜ì  (Floating-Point Number)

* ìˆ˜ë¥¼ **ê°€ìˆ˜(M) Ã— ë°‘(B)^ì§€ìˆ˜(E)** ë¡œ í‘œí˜„
* IEEE 754 í‘œì¤€:

  * **Single (float, 32bit)** â†’ 1(sign) + 8(exponent) + 23(mantissa)
  * **Double (64bit)** â†’ 1 + 11 + 52
* Bias(í¸í–¥ ì§€ìˆ˜) ë„ì…:
  `E_true = E - Bias` (Bias = 2^(eâˆ’1)âˆ’1)
* íŠ¹ì§•:

  * ì§€ìˆ˜ë¡œ í‘œí˜„ ë²”ìœ„ í™•ëŒ€
  * Hidden bit(1.xxx) ì‚¬ìš© â†’ ì •ë°€ë„ â†‘
  * 0, ë¬´í•œëŒ€, NaN ì •ì˜

> ğŸ’¬ â€œì»´í“¨í„°ì—ì„œ ìˆ˜ë¥¼ í‘œí˜„í•˜ëŠ” íš¨ìœ¨ì´ ê³§ ì—°ì‚° íš¨ìœ¨ì´ë‹¤.â€

---

## 3ï¸âƒ£ ëª¨ë¸ ê²½ëŸ‰í™” (Model Compression)

### ğŸš§ í•„ìš”ì„±

* ëŒ€í˜• ëª¨ë¸ì˜ **íŒŒë¼ë¯¸í„° ìˆ˜ â†’ ì—°ì‚°ëŸ‰, ë©”ëª¨ë¦¬, ì „ë ¥** ëª¨ë‘ í­ì¦
* GPU ë¦¬ì†ŒìŠ¤ í•œê³„ë¡œ **íš¨ìœ¨í™”ëŠ” í•„ìˆ˜ ê³¼ì œ**
* ê·¸ëŸ¬ë‚˜ ì••ì¶•ë¥ ì´ ë†’ì„ìˆ˜ë¡ ì •í™•ë„ ì €í•˜ ë°œìƒ â†’ ê· í˜• ì„¤ê³„ í•„ìš”

---

### ğŸ§© ì£¼ìš” ê¸°ë²• 3ëŒ€ ì¶•

| ê¸°ë²•               | í•µì‹¬ ê°œë…          | íŠ¹ì§•             |
| ---------------- | -------------- | -------------- |
| **Quantization** | ìˆ˜ì¹˜ ì •ë°€ë„ ë‚®ì¶”ê¸°     | ì†ë„â†‘, ë©”ëª¨ë¦¬â†“      |
| **Pruning**      | ì¤‘ìš”í•˜ì§€ ì•Šì€ ê°€ì¤‘ì¹˜ ì œê±° | í¬ê¸°â†“, ì¼ë¶€ ì •í™•ë„ ì†ì‹¤ |
| **Distillation** | ëŒ€í˜• ëª¨ë¸ ì§€ì‹ ì „ìˆ˜    | ê²½ëŸ‰ ëª¨ë¸ ì„±ëŠ¥â†‘      |

---

## 4ï¸âƒ£ Quantization (ì–‘ìí™”)

### ğŸ’¡ ê°œë…

* **ì—°ì‚° ë‹¨ìœ„ë¥¼ float â†’ int** ë¡œ ë³€í™˜í•˜ì—¬ ê³„ì‚°ëŸ‰ ë° ë©”ëª¨ë¦¬ ì ˆê°
* ì¼ë°˜ì ìœ¼ë¡œ FP32 â†’ INT8 ë˜ëŠ” INT4ë¡œ ì¶•ì†Œ
* ì—°ì‚° ë‹¨ìˆœí™”ë¡œ **ì¶”ë¡  ì†ë„ ë° ì—ë„ˆì§€ íš¨ìœ¨ ê°œì„ **

---

### âš™ï¸ êµ¬í˜„ ë°©ì‹

| êµ¬ë¶„                                    | ì„¤ëª…                                |
| ------------------------------------- | --------------------------------- |
| **QAT (Quantization-Aware Training)** | í•™ìŠµ ì¤‘ ì–‘ìí™” ê³ ë ¤ â†’ ì •í™•ë„ ì†ì‹¤ ìµœì†Œí™”, í•™ìŠµë¹„ìš© ë†’ìŒ |
| **PTQ (Post-Training Quantization)**  | í•™ìŠµ í›„ ì–‘ìí™” ìˆ˜í–‰ â†’ LLM ë“± ëŒ€ê·œëª¨ ëª¨ë¸ì— ì£¼ë¡œ ì‚¬ìš© |

---

### ğŸ”¹ ì–‘ìí™” ìœ í˜•

| êµ¬ë¶„                    | ì„¤ëª…                            |
| --------------------- | ----------------------------- |
| **Weight-only**       | ê°€ì¤‘ì¹˜ë§Œ ì •ìˆ˜ ë³€í™˜ â†’ ì •í™•ë„ ì†ì‹¤ ì ìŒ        |
| **Weight+Activation** | ê°€ì¤‘ì¹˜ + í™œì„±ê°’ ëª¨ë‘ ë³€í™˜ â†’ ì†ë„ â†‘, ì •í™•ë„ â†“ |

---

### âš–ï¸ Symmetric vs Asymmetric

| ì¢…ë¥˜             | íŠ¹ì§•                                  |
| -------------- | ----------------------------------- |
| **Symmetric**  | 0 ì¤‘ì‹¬ ëŒ€ì¹­ ë²”ìœ„, ì—°ì‚° ë‹¨ìˆœ, GPU íš¨ìœ¨ì           |
| **Asymmetric** | scale + zero-point ì‚¬ìš©, ë°ì´í„° ë¶„í¬ ì™œê³¡ ì ìŒ |

---

### ğŸ”¸ ì •ë°€ë„ í‘œí˜„

| í¬ë§·   | ë¹„íŠ¸êµ¬ì„±         | íŠ¹ì§•                |
| ---- | ------------ | ----------------- |
| FP32 | E8M23        | í‘œì¤€ ë¶€ë™ì†Œìˆ˜ì           |
| BF16 | E8M7         | ë²”ìœ„ ìœ ì§€ + ë©”ëª¨ë¦¬ ì ˆê°    |
| TF32 | E8M10        | FP16 ì†ë„ + FP32 ë²”ìœ„ |
| FP8  | E4M3 or E5M2 | ì´ˆê²½ëŸ‰í˜•, ìµœì‹  GPU ì „ìš©   |

---

### âš™ï¸ Mixed-Precision Quantization

* Layerë³„ë¡œ ì„œë¡œ ë‹¤ë¥¸ ì •ë°€ë„ ì ìš©
* ì„±ëŠ¥ê³¼ ì†ë„ì˜ ê· í˜• ì¡°ì • ê°€ëŠ¥

> ğŸ’¬ "ì–‘ìí™”ëŠ” ëª¨ë¸ ì••ì¶• ì¤‘ ê°€ì¥ ë¹ ë¥´ê³  GPU ì¹œí™”ì ì¸ ì ‘ê·¼ì´ë‹¤."

---

## 5ï¸âƒ£ Pruning (ê°€ì§€ì¹˜ê¸°)

### ğŸ“˜ ì •ì˜

* ë¶ˆí•„ìš”í•œ weight(ì‘ì€ ì¤‘ìš”ë„)ë¥¼ ì œê±°í•˜ì—¬ ëª¨ë¸ í¬ê¸°ë¥¼ ì¤„ì´ëŠ” ê¸°ë²•
* ì¢…ë¥˜:

  * **Unstructured**: ì„ì˜ ê°€ì¤‘ì¹˜ ì œê±° (íš¨ìœ¨â†‘, í•˜ë“œì›¨ì–´ í™œìš©â†“)
  * **Structured**: ì±„ë„/í•„í„° ë‹¨ìœ„ ì œê±° (GPU ê°€ì† í˜¸í™˜â†‘)

---

### âš™ï¸ ë°©ì‹

| ìœ í˜•                 | ì„¤ëª…                |
| ------------------ | ----------------- |
| Magnitude-based    | ì ˆëŒ“ê°’ì´ ì‘ì€ weight ì œê±° |
| Momentum-based     | ìµœê·¼ ë³€í™”ìœ¨ ê³ ë ¤         |
| Structured pruning | ì±„ë„ ë‹¨ìœ„ ì œê±°          |

> ğŸ’¡ CNNì€ pruning íš¨ìœ¨ì´ ë†’ê³ , LLMì€ pruningë¥ ì´ ì œí•œì  (ì•½ 50%)

---

## 6ï¸âƒ£ Knowledge Distillation (ì§€ì‹ ì¦ë¥˜)

### ğŸ§  ì›ë¦¬

* **Teacher (ëŒ€í˜• ëª¨ë¸)** â†’ **Student (ì†Œí˜• ëª¨ë¸)** ë¡œ ì§€ì‹ ì „ìˆ˜
* í•™ìŠµ ì‹œ â€œSoft Label (í™•ë¥  ë¶„í¬)â€ ì‚¬ìš©
* ì†ì‹¤í•¨ìˆ˜:
  [
  L = L_{student} + Î»L_{distill}
  ]

---

### ğŸ’¬ íš¨ê³¼ì  ë°©ë²•

* Teacherâ€“Assistantâ€“Student êµ¬ì¡°(TAKD) í™œìš©
  â†’ ëŒ€í˜• ëª¨ë¸ê³¼ ì‘ì€ ëª¨ë¸ ì‚¬ì´ì— **ì¡°êµ ëª¨ë¸(Assistant)** ì¶”ê°€ ì‹œ ì„±ëŠ¥ â†‘
* LLMì—ì„œë„ í™œë°œíˆ ì‚¬ìš© (GPT â†’ Alpaca, TinyLlama ë“±)

---

## 7ï¸âƒ£ íŒŒë¼ë¯¸í„° íš¨ìœ¨ì  íŒŒì¸íŠœë‹ (PEFT)

### ğŸ’¡ í•µì‹¬ ê°œë…

> "ëª¨ë“  íŒŒë¼ë¯¸í„°ë¥¼ ì¬í•™ìŠµí•˜ì§€ ì•Šê³ , ì¼ë¶€ë§Œ ìˆ˜ì •í•˜ì—¬ íš¨ìœ¨ì ìœ¼ë¡œ íŠœë‹í•˜ëŠ” ê¸°ë²•"

---

### ğŸ”¹ ì£¼ìš” ë°©ì‹

| ë°©ì‹                | ì„¤ëª…                     | íŠ¹ì§•                |
| ----------------- | ---------------------- | ----------------- |
| **Adapter Layer** | MLP Layer ì¶”ê°€           | êµ¬ì¡° ë³€ê²½, í‘œí˜„ë ¥â†‘, ë©”ëª¨ë¦¬â†‘ |
| **Prompt Tuning** | í•™ìŠµ ê°€ëŠ¥í•œ ê°€ì§œ í† í° ì‚½ì…        | ê²½ëŸ‰Â·ë¹ ë¦„, ì„±ëŠ¥ ì œí•œ      |
| **LoRA**          | Weightì— Low-Rank í–‰ë ¬ ì¶”ê°€ | ì ì€ íŒŒë¼ë¯¸í„°ë¡œ ë†’ì€ í‘œí˜„ë ¥   |
| **QLoRA**         | Quantization + LoRA ê²°í•© | GPU ë©”ëª¨ë¦¬â†“, ì„±ëŠ¥â†‘     |

---

### ğŸ§© LoRA ì›ë¦¬

[
W' = W + BA
]

* W: ê¸°ì¡´ weight (ê³ ì •)
* A, B: í•™ìŠµ ê°€ëŠ¥í•œ ì €ë­í¬ í–‰ë ¬ (rank â‰ª dim)
* ì¥ì :

  * ì¶”ê°€ íŒŒë¼ë¯¸í„° ê·¹ì†Œí™”
  * ê¸°ì¡´ weight ë¶ˆë³€ â†’ ì›ë³¸ ëª¨ë¸ ë³´ì¡´
  * Full Fine-tuning ëŒ€ë¹„ ì†ë„Â·ë©”ëª¨ë¦¬ ì ˆê°

---

### âš™ï¸ QLoRA

* ì›ë³¸ ëª¨ë¸ì„ **4bit ì–‘ìí™”(NF4)** í•˜ì—¬ ì €ì¥
* LoRA ë¶€ë¶„ì€ **BF16 ì •ë°€ë„** ìœ ì§€
* GPU ë©”ëª¨ë¦¬ ì ˆê° (ê¸°ì¡´ ëŒ€ë¹„ 75%â†“)
* ëŒ€í˜• ëª¨ë¸ë„ ì¼ë°˜ GPUë¡œ íŒŒì¸íŠœë‹ ê°€ëŠ¥

> ğŸ’¡ ëŒ€í‘œ ë¼ì´ë¸ŒëŸ¬ë¦¬: **Unsloth**, **Axolotl**

---

## âœ… í•µì‹¬ ìš”ì•½

| ì£¼ì œ           | í•µì‹¬ í‚¤ì›Œë“œ                                      |
| ------------ | ------------------------------------------- |
| ìˆ˜ ì²´ê³„         | ì´ì§„ìˆ˜, ê³ ì •/ë¶€ë™ì†Œìˆ˜ì , IEEE 754                     |
| Quantization | FP32â†’INT8, Symmetric/Asymmetric, QAT/PTQ    |
| Pruning      | Unstructured vs Structured, Magnitude-based |
| Distillation | Teacherâ€“Student êµ¬ì¡°, TAKD                    |
| PEFT         | Adapter, Prompt Tuning, LoRA, QLoRA         |
| QLoRA        | NF4 ì–‘ìí™” + BF16 LoRA, íš¨ìœ¨ ìµœê³                   |

