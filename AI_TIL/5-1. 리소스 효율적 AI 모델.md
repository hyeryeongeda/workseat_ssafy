## 1️⃣ 학습 목표

* 컴퓨터가 수를 이해하고 처리하는 **이진수 체계** 이해
* **고정소수점 / 부동소수점** 구조의 차이와 연산 복잡도 이해
* **모델 경량화(Quantization, Pruning, Distillation)** 기법의 원리와 활용
* **파라미터 효율적 파인튜닝 (PEFT)** 개념 및 **LoRA / QLoRA** 의 작동 원리 이해

---

## 2️⃣ 컴퓨터의 수 체계 (Binary Number System)

### ⚙️ 2진수 기본 구조

* 2진수는 0과 1만을 사용하며, 각 자릿수는 2의 지수를 나타냄
* `2^10 ≈ 1,000`, `2^20 ≈ 1,000,000`, `2^30 ≈ 1,000,000,000`

---

### 🧮 고정소수점 (Fixed-Point Number)

* N-bit 내에서 **정수부 + 소수부**를 함께 표현
* 소수점 위치는 고정되지 않고 “사용자가 지정” (scaling factor 사용)
* 계산 예:

  ```
  (4,4) 8bit 고정소수점 → 4bit 정수, 4bit 소수
  3.5 + 1.25 = 4.75
  실제 연산: (3.5×16 + 1.25×16) / 16 = 76/16 = 4.75
  ```
* 💡 단점: 큰 수/작은 수 표현 시 비트 낭비 큼 → 부동소수점 등장

---

### 📘 부동소수점 (Floating-Point Number)

* 수를 **가수(M) × 밑(B)^지수(E)** 로 표현
* IEEE 754 표준:

  * **Single (float, 32bit)** → 1(sign) + 8(exponent) + 23(mantissa)
  * **Double (64bit)** → 1 + 11 + 52
* Bias(편향 지수) 도입:
  `E_true = E - Bias` (Bias = 2^(e−1)−1)
* 특징:

  * 지수로 표현 범위 확대
  * Hidden bit(1.xxx) 사용 → 정밀도 ↑
  * 0, 무한대, NaN 정의

> 💬 “컴퓨터에서 수를 표현하는 효율이 곧 연산 효율이다.”

---

## 3️⃣ 모델 경량화 (Model Compression)

### 🚧 필요성

* 대형 모델의 **파라미터 수 → 연산량, 메모리, 전력** 모두 폭증
* GPU 리소스 한계로 **효율화는 필수 과제**
* 그러나 압축률이 높을수록 정확도 저하 발생 → 균형 설계 필요

---

### 🧩 주요 기법 3대 축

| 기법               | 핵심 개념          | 특징             |
| ---------------- | -------------- | -------------- |
| **Quantization** | 수치 정밀도 낮추기     | 속도↑, 메모리↓      |
| **Pruning**      | 중요하지 않은 가중치 제거 | 크기↓, 일부 정확도 손실 |
| **Distillation** | 대형 모델 지식 전수    | 경량 모델 성능↑      |

---

## 4️⃣ Quantization (양자화)

### 💡 개념

* **연산 단위를 float → int** 로 변환하여 계산량 및 메모리 절감
* 일반적으로 FP32 → INT8 또는 INT4로 축소
* 연산 단순화로 **추론 속도 및 에너지 효율 개선**

---

### ⚙️ 구현 방식

| 구분                                    | 설명                                |
| ------------------------------------- | --------------------------------- |
| **QAT (Quantization-Aware Training)** | 학습 중 양자화 고려 → 정확도 손실 최소화, 학습비용 높음 |
| **PTQ (Post-Training Quantization)**  | 학습 후 양자화 수행 → LLM 등 대규모 모델에 주로 사용 |

---

### 🔹 양자화 유형

| 구분                    | 설명                            |
| --------------------- | ----------------------------- |
| **Weight-only**       | 가중치만 정수 변환 → 정확도 손실 적음        |
| **Weight+Activation** | 가중치 + 활성값 모두 변환 → 속도 ↑, 정확도 ↓ |

---

### ⚖️ Symmetric vs Asymmetric

| 종류             | 특징                                  |
| -------------- | ----------------------------------- |
| **Symmetric**  | 0 중심 대칭 범위, 연산 단순, GPU 효율적          |
| **Asymmetric** | scale + zero-point 사용, 데이터 분포 왜곡 적음 |

---

### 🔸 정밀도 표현

| 포맷   | 비트구성         | 특징                |
| ---- | ------------ | ----------------- |
| FP32 | E8M23        | 표준 부동소수점          |
| BF16 | E8M7         | 범위 유지 + 메모리 절감    |
| TF32 | E8M10        | FP16 속도 + FP32 범위 |
| FP8  | E4M3 or E5M2 | 초경량형, 최신 GPU 전용   |

---

### ⚙️ Mixed-Precision Quantization

* Layer별로 서로 다른 정밀도 적용
* 성능과 속도의 균형 조정 가능

> 💬 "양자화는 모델 압축 중 가장 빠르고 GPU 친화적인 접근이다."

---

## 5️⃣ Pruning (가지치기)

### 📘 정의

* 불필요한 weight(작은 중요도)를 제거하여 모델 크기를 줄이는 기법
* 종류:

  * **Unstructured**: 임의 가중치 제거 (효율↑, 하드웨어 활용↓)
  * **Structured**: 채널/필터 단위 제거 (GPU 가속 호환↑)

---

### ⚙️ 방식

| 유형                 | 설명                |
| ------------------ | ----------------- |
| Magnitude-based    | 절댓값이 작은 weight 제거 |
| Momentum-based     | 최근 변화율 고려         |
| Structured pruning | 채널 단위 제거          |

> 💡 CNN은 pruning 효율이 높고, LLM은 pruning률이 제한적 (약 50%)

---

## 6️⃣ Knowledge Distillation (지식 증류)

### 🧠 원리

* **Teacher (대형 모델)** → **Student (소형 모델)** 로 지식 전수
* 학습 시 “Soft Label (확률 분포)” 사용
* 손실함수:
  [
  L = L_{student} + λL_{distill}
  ]

---

### 💬 효과적 방법

* Teacher–Assistant–Student 구조(TAKD) 활용
  → 대형 모델과 작은 모델 사이에 **조교 모델(Assistant)** 추가 시 성능 ↑
* LLM에서도 활발히 사용 (GPT → Alpaca, TinyLlama 등)

---

## 7️⃣ 파라미터 효율적 파인튜닝 (PEFT)

### 💡 핵심 개념

> "모든 파라미터를 재학습하지 않고, 일부만 수정하여 효율적으로 튜닝하는 기법"

---

### 🔹 주요 방식

| 방식                | 설명                     | 특징                |
| ----------------- | ---------------------- | ----------------- |
| **Adapter Layer** | MLP Layer 추가           | 구조 변경, 표현력↑, 메모리↑ |
| **Prompt Tuning** | 학습 가능한 가짜 토큰 삽입        | 경량·빠름, 성능 제한      |
| **LoRA**          | Weight에 Low-Rank 행렬 추가 | 적은 파라미터로 높은 표현력   |
| **QLoRA**         | Quantization + LoRA 결합 | GPU 메모리↓, 성능↑     |

---

### 🧩 LoRA 원리

[
W' = W + BA
]

* W: 기존 weight (고정)
* A, B: 학습 가능한 저랭크 행렬 (rank ≪ dim)
* 장점:

  * 추가 파라미터 극소화
  * 기존 weight 불변 → 원본 모델 보존
  * Full Fine-tuning 대비 속도·메모리 절감

---

### ⚙️ QLoRA

* 원본 모델을 **4bit 양자화(NF4)** 하여 저장
* LoRA 부분은 **BF16 정밀도** 유지
* GPU 메모리 절감 (기존 대비 75%↓)
* 대형 모델도 일반 GPU로 파인튜닝 가능

> 💡 대표 라이브러리: **Unsloth**, **Axolotl**

---

## ✅ 핵심 요약

| 주제           | 핵심 키워드                                      |
| ------------ | ------------------------------------------- |
| 수 체계         | 이진수, 고정/부동소수점, IEEE 754                     |
| Quantization | FP32→INT8, Symmetric/Asymmetric, QAT/PTQ    |
| Pruning      | Unstructured vs Structured, Magnitude-based |
| Distillation | Teacher–Student 구조, TAKD                    |
| PEFT         | Adapter, Prompt Tuning, LoRA, QLoRA         |
| QLoRA        | NF4 양자화 + BF16 LoRA, 효율 최고                  |

