## 🧠 **AI & 기계학습 방법**

### 1️⃣ 선형 회귀 (Linear Regression)

**📘 개념**

* 입력 변수 ( X_1, X_2, …, X_p ) 가 출력 ( Y )에 **선형적으로 의존**한다고 가정
* 예: 광고비가 많을수록 매출이 증가

**📊 단순 선형 회귀 (Simple Linear Regression)**
[
Y = β_0 + β_1 X + ε
]

* β₀: 절편, β₁: 기울기
* RSS (Residual Sum of Squares, 잔차 제곱합) 최소화 → 최적 β 추정
* RSS = Σ(yi − ŷi)²

**📉 예시**

* TV 광고비와 매출의 선형 관계

  * R² = 0.612 → 매출 변동의 61.2%를 광고비로 설명
  * p-value < 0.0001 → 유의한 관계

**📈 다중 선형 회귀 (Multiple Linear Regression)**
[
Y = β_0 + β_1 X_1 + β_2 X_2 + … + β_p X_p + ε
]

* 여러 입력 변수를 동시에 고려
* 해석: 다른 변수들을 고정했을 때 Xj가 1단위 증가 → Y는 평균적으로 βj만큼 변함
* 행렬로 표현 가능, 최소제곱 해로 추정

---

### 2️⃣ 로지스틱 회귀 (Logistic Regression)

**📘 분류(Classification)**

* 출력 Y가 **범주형**(ex. 스팸/정상, 연체/비연체)
* 회귀 예측값을 확률로 해석할 때 **0~1 범위**로 제한해야 함

**📊 로지스틱 회귀식**
[
P(Y=1|X) = \frac{1}{1 + e^{-(β_0 + β_1 X)}}
]

* S자 형태의 **시그모이드 함수**
* X가 커질수록 1에 가까워지고, 작을수록 0에 가까움
* MLE(Maximum Likelihood Estimation)로 파라미터 추정

**📍 예시: 신용카드 연체 예측**

* Balance, Income 등 입력 변수로 연체 확률 추정
* 단순한 예/아니오 분류보다 “연체 확률” 자체를 예측하는 것이 유용

---

### 3️⃣ 신경망 모델 (Neural Network)

**📘 개념**

* 선형회귀를 확장한 **비선형 함수 근사기**
* 입력 → 은닉층(hidden layer) → 출력
* 활성화 함수(ReLU 등)를 통해 **비선형성** 추가

**🧩 Shallow Network (은닉층 1개)**

* 활성화 함수: ReLU (x>0이면 x, x≤0이면 0)
* Piecewise Linear Function: 구간별 선형 함수
* Hidden Unit이 많을수록 더 복잡한 함수 근사 가능
* **보편 근사정리**: 은닉층 1개라도 충분히 많은 유닛이 있으면 임의의 연속함수를 근사 가능

**🏗 Deep Network (은닉층 2개 이상)**

* 여러 Shallow Network를 합성 → 더 복잡한 패턴 학습 가능
* 표현력은 증가하지만 수식과 최적화 복잡도도 증가

---

### ⚙️ **경사하강법 (Gradient Descent)**

**손실함수(Loss Function)**: 예측값과 실제값의 차이
**업데이트 규칙:**
[
θ_{new} = θ_{old} - α \cdot \nabla_θ L(θ)
]

* α: 학습률(learning rate)
* L: 손실 함수

**유형**

| 방법                 | 설명             | 특징         |
| ------------------ | -------------- | ---------- |
| **Batch GD**       | 전체 데이터로 기울기 계산 | 안정적, 느림    |
| **SGD**            | 데이터 1개씩 계산     | 빠르지만 진동 심함 |
| **Mini-batch SGD** | 일부 샘플로 계산      | 속도·안정성 균형  |

**Epoch**: 전체 데이터를 1회 학습하는 주기

---

## 📚 핵심 요약표

| 구분         | 주요 개념       | 핵심 식                      | 특징        |
| ---------- | ----------- | ------------------------- | --------- |
| 선형 회귀      | Y와 X의 선형 관계 | Y = β₀ + β₁X + ε          | 연속형 예측    |
| 로지스틱 회귀    | 확률 기반 이진 분류 | p = 1/(1+e⁻ᶻ)             | 0~1 확률 예측 |
| Shallow NN | 은닉층 1개      | y = f(W₂·ReLU(W₁x+b₁)+b₂) | 비선형 근사    |
| Deep NN    | 은닉층 2개 이상   | y = f₂(f₁(x))             | 고차원 패턴 학습 |
| 경사하강법      | 손실 최소화      | θ ← θ - α∇L(θ)            | 최적화 알고리즘  |

